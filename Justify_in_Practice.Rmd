---
title             : "Justify Your Alpha: A Primer on Two Practical Approaches"
shorttitle        : "Justify in Practice"
author: 
  - name          : "Maximilian Maier"
    affiliation   : "1"
    corresponding : no
  - name          : "Daniël Lakens"
    affiliation   : "2"
    corresponding : yes
    address       : "ATLAS 9.402, 5600 MB, Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
affiliation:
  - id            : "1"
    institution   : "University College London, United Kingdom"
  - id            : "2"
    institution   : "Eindhoven University of Technology, The Netherlands"

authornote: |
  We thank Eric-Jan Wagenmakers for feedback on a previous version of the manuscript that helped improve the section on the Jeffreys-Lindley paradox. 

  Author contributions: M. Maier and D. Lakens jointly wrote and edited the manuscript. D. Lakens focused more strongly on minimizing or balancing error rates and M. Maier focused more strongly on justifying the alpha level as a function of sample size. D. Lakens and M. Maier both contributed to the R-package and the shiny app. 

  Conflicts of interest: The authors declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

abstract: |
 The default use of an alpha level of 0.05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power *p*-values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley's paradox). This manuscript explains two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of 0.05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors), but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.
  
keywords          : "Hypothesis Testing, Type 1 Error, Type 2 Error, Statistical Power"
wordcount         : 7325 words
bibliography      : ["justify_alpha_in_practice.bib"]
figsintext        : yes
linenumbers       : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
class             : "man, a4paper"
output            : papaja::apa6_pdf
header-includes:
  - \raggedbottom
latex_engine      : xelatex
  - \usepackage[T1]{fontenc}
  - \usepackage{heuristica}
  - \usepackage{xcolor}
mainfont: heuristica

---
```{r load_packages, include=FALSE}
if(!require(JustifyAlpha)){devtools::install_github("Lakens/JustifyAlpha")}
library(JustifyAlpha)
library(ggplot2)
library(pwr)
library(papaja)
```



\textcolor{black}{Scientists regularly need to make dichotomous decisions when they perform lines of research. Should a pilot study be performed, or not? When multiple possible manipulations or measures are available, which should be used for the next study? Should the design of a study include a possible moderator, or can it be ignored? Should a research line be continued, or abandoned? These decisions come with costs and benefits for the scientist, as well as for society, when bad decisions lead to research waste.} In a Neyman-Pearson approach to hypothesis testing [@neyman_problem_1933] studies are designed such that erroneous decisions that determine how we act are controlled in the long run at some desired maximum level. If resources were infinite we could collect enough data to make the chance of a wrong decision incredibly small \textcolor{black}{by using an extremely low alpha level while still achieving very high statistical power.} However, since resources are limited, researchers need to decide how to choose the rate at which they are willing to make errors [@wald1949statistical]. After data is collected researchers can incorrectly act as if there is an effect when there is no true effect (a Type 1 error) or incorrectly act as if there is no effect when there is a true effect (a Type 2 error). With the same number of observations, a reduction in the Type 1 error rate will increase the Type 2 error rate (and vice versa). 

The question how error rates should be set in any study requires careful consideration of the relative costs of a Type 1 error or a Type 2 error. Regrettably, researchers rarely provide such a justification and predominantly use an alpha level of 5%. In the past, the strong convention to use a 5% alpha level might have functioned as a de facto prespecification of the alpha level, which \textcolor{black}{was useful given that the alpha level needs to be decided upon} before the data is analyzed [@tunc_epistemic_2021]. Nowadays, researchers can transparently preregister a statistical analysis plan in an online repository, which makes it possible to specify more appropriate but less conventional alpha levels. Even though it is possible to preregister non-conventional alpha levels, there is relatively little practical guidance on how to choose an alpha level for a study. This article explains why error rates need to be justified and provides two practical approaches that can be used to justify the alpha level. In the first approach the cost of Type I and Type II error rates are balanced or minimized and in the second approach the alpha level is lowered as a function of the sample size. 

# Why Do We Use a 5% Alpha Level and 80% Power?

We might naively assume that when all researchers do something, there must be a good reason for such an established practice. An important step towards maturity as a scholar is the realization that this is not the case. Neither Fisher nor Neyman, two statistical giants largely responsible for the widespread reliance on hypothesis tests in the social sciences, recommended the universal use of any specific threshold. @fisher_design_1971 writes: "It is open to the experimenter to be more or less exacting in respect of the smallness of the probability he would require before he would be willing to admit that his observations have demonstrated a positive result." Similarly, @neyman_problem_1933 write: "From the point of view of mathematical theory all that we can do is to show how the risk of the errors may be controlled and minimized. The use of these statistical tools in any given case, in determining just how the balance should be struck, must be left to the investigator."

Even though in *theory* alpha levels should be justified, in *practice* researchers tend to imitate others. @fisher_introduction_1926 notes: "Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fail to reach this level". This sentence is preceded by the statement "If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty (the 2 percent point), or one in a hundred (the 1 percent point)." Indeed, in his examples Fisher often uses an alpha of 0.01. Nevertheless, researchers have copied the value Fisher preferred, instead of his more important take-home message that the significance level should be set by the experimenter. The default use of an alpha level of 0.05 \textcolor{black}{can already be found in work} of Gosset on the *t*-distribution [@cowles_origins_1982; @kennedy-shaffer_before_2019], who believed that a difference of two standard deviations (a z-score of 2) was sufficiently rare.

The default use of 80% power (or a 20% Type 2, or beta (b) error) is similarly based on personal preferences by @cohen_statistical_1988, who writes: "It is proposed here as a convention that, when the investigator has no other basis for setting the desired power value, the value .80 be used. This means that beta is set at .20. This value is offered for several reasons (Cohen, 1965, pp. 98-99). The chief among them takes into consideration the implicit convention for alpha of .05. The beta of .20 is chosen with the idea that the general relative seriousness of these two kinds of errors is of the order of .20/.05, i.e., that Type I errors are of the order of four times as serious as Type II errors. This .80 desired power convention is offered with the hope that it will be ignored whenever an investigator can find a basis in his substantive concerns in his specific research investigation to choose a value ad hoc."

We see that conventions are built on conventions: the norm to aim for 80% power is built on the norm to set the alpha level at 5%. \textcolor{black}{This normative use of statistics was criticized in a statement by the American Statistical Association} [@wasserstein2016asa], \textcolor{black}{who wrote: ``We teach it because it’s what we do; we do it because it’s what we teach.''} The real lesson we should take away from Cohen is to determine the relative seriousness of Type 1 and Type 2 errors, and to balance both types of errors when a study is designed. If a Type 1 error is considered to be four times as serious as a Type 2 error, the *weighted* error rates in the study are balanced with a 5% Type 1 error rate and a 20% Type 2 error rate. 

## Justifying the Alpha Level

In 1957 Neyman wrote: "it appears desirable to determine the level of significance in accordance with quite a few circumstances that vary from one particular problem to the next" [@neyman_1957]. Despite this advice, the mindless application of null hypothesis significance tests, including setting the alpha level at 5% for all tests, is so universal that it has been criticized for more than half a century [@bakan_test_1966; @gigerenzer_statistical_2018]. The default use of a 5% alpha level might have been difficult to abandon, even if it was a mediocre research practice, without an alternative approach in which alpha levels are better justified.

There are two main reasons to abandon the universal use of a 5% alpha level. The first reason to carefully choose an alpha level is that decision-making becomes more efficient [@mudge_setting_2012]. If researchers use hypothesis tests to make dichotomous decisions from a methodological falsificationist approach to statistical inferences [@tunc_epistemic_2021], and have a certain maximum sample size they are willing or able to collect, it is typically possible to make decisions more efficiently by choosing error rates such that the combined cost of Type 1 and Type 2 errors is minimized. If we aim to either minimize or balance Type 1 and Type 2 error rates for a given sample size and effect size, the alpha level should be set not based on convention, but by weighting the relative cost of both types of errors.

The second reason is most relevant for large data sets [@harford2014big]. As the statistical power increases, some *p*-values below 0.05 (e.g., *p* = 0.04) can be more likely when there is *no* effect than when there *is* an effect. This is known as Lindley's paradox [@lindley_statistical_1957; @cousins_jeffreyslindley_2017; @jeffreys1935some; @bartlett1957comment; @jeffreys1936on; @jeffreys1936further; @lin2013research], or sometimes the Jeffreys-Lindley paradox [@spanos_2013], as Harold Jeffreys discussed the paradox long before Lindley [@WagenmakersJeffreys]. The distribution of *p*-values is a function of the statistical power [@cumming_replication_2008], and the higher the power, the more right-skewed the distribution becomes (i.e., the more likely it becomes that small *p*-values are observed). When there is no true effect *p*-values are uniformly distributed, and 1% of observed *p*-values fall between 0.04 and 0.05. When the statistical power is extremely high, not only will most *p*-values fall below 0.05, most will also fall below 0.01. In Figure \ref{fig:p-plot} we see that with high power very small *p*-values are more likely to be observed when there *is* an effect than when there is *no* effect (e.g., the red curve representing *p*-values when the alternative is true falls above the dashed horizontal line for a *p*-value of 0.01). But observing a *p*-value of 0.04 is more likely when the null hypothesis (H0) is true than when the alternative hypothesis (H1) is true and we have very high power, \textcolor{black}{as illustrated by the fact that the density of the $p$-value distribution is higher under H0 than under H1 at 0.04 in Figure \ref{fig:p-plot}.}

```{r p-plot, fig.width=10, fig.height=6, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="*P*-value distributions for a two-sided independent *t*-test with N = 150 and d = 0.5 (red curve) or d = 0 (horizontal dashed line) which illustrates how *p*-values just below 0.05 can be more likely when there is no effect than when there is an effect."}
N <- 150
d <- 0.5
p <- 0.05
p_upper <- 0.05 + 0.00000001
p_lower <- 0.00000001
ymax <- 10 # Maximum value y-scale (only for p-curve)

# Calculations
se <- sqrt(2 / N) # standard error
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d

# p-value function
pdf2_t <- function(p) 0.5 * dt(qt(p / 2, 2 * N - 2, 0), 2 * N - 2, ncp) / dt(qt(p / 2, 2 * N - 2, 0), 2 * N - 2, 0) + dt(qt(1 - p / 2, 2 * N - 2, 0), 2 * N - 2, ncp) / dt(qt(1 - p / 2, 2 * N - 2, 0), 2 * N - 2, 0)
plot(-10,
  xlab = "P-value", ylab = "", axes = FALSE,
  main = "P-value distribution for d = 0.5 and N = 150", xlim = c(0, .1), ylim = c(0, ymax), cex.lab = 1.5, cex.main = 1.5, cex.sub = 1.5
)
axis(side = 1, at = seq(0, 1, 0.01), labels = seq(0, 1, 0.01), cex.axis = 1.5)
# cord.x <- c(p_lower,seq(p_lower,p_upper,0.001),p_upper)
# cord.y <- c(0,pdf2_t(seq(p_lower, p_upper, 0.001)),0)
# polygon(cord.x,cord.y,col=rgb(0.5, 0.5, 0.5,0.5))
curve(pdf2_t, 0, .1, n = 1000, col = "red", lwd = 3, add = TRUE)
ncp <- (0 * sqrt(N / 2)) # Calculate non-centrality parameter d
curve(pdf2_t, 0, 1, n = 1000, col = "black", lty = 2, lwd = 3, add = TRUE)
abline(v = 0.05, col = "black", lty = 3, lwd = 3)
```

Although it is not necessary from a Neyman-Pearson error-statistical perspective, researchers often want to interpret a significant test result as evidence for the alternative hypothesis. In other words, in addition to controlling the *error rate*, researchers might be interested in interpreting the *relative evidence* in the data for the alternative hypothesis over the null hypothesis. If so, it makes sense to choose the alpha level such that when a significant *p*-value is observed, the *p*-value is actually more likely when the alternative hypothesis is true than when the null hypothesis is true. This means that when statistical power is very high (e.g., the sample size is very large), the alpha level should be reduced. For example, if the alpha level in Figure \ref{fig:p-plot} is lowered to 0.02 then the alternative hypothesis is more likely than the null hypothesis for all significant *p*-values that would be observed. This approach to justifying the alpha level can be seen as a frequentist/Bayesian compromise [@good_bayes-non-bayes_1992]. The error rate is controlled, but at the same time the alpha level is set to a value that guarantees that whenever we reject the null hypothesis, the data is more likely under the alternative hypothesis than under the null. 

## Minimizing or Balancing Type 1 and Type 2 Error Rates

If both Type 1 as Type 2 errors are costly, then it makes sense to optimally reduce both errors as you design studies. \textcolor{black}{This idea is well established in applied statistics} [@degroot1975probability; @pericchi2016adaptative; @lindley1953statistical; @mudge_setting_2012; @cornfield1969bayesian; @kim2021choosing] and leads to studies where you make decisions most efficiently. Researchers can choose to design a study with a statistical power and alpha level that minimizes the *weighted combined error rate*. For example, a researcher designs an experiment where they assume H0 and H1 are a-priori equally probable (the prior probability for both is 0.5). They set the Type 1 error rate to 0.05 and collect sufficient data such that the statistical power is 0.80. The weighted combined error rate is 0.5 (the probability H0 is true) × 0.05 (the probability of a Type 1 error) + 0.5 (the probability that H1 is true) × 0.20 (the probability of a Type 2 error) = 0.125. This weighted combined error rate might be lower if a different choice for Type 1 and Type 2 errors was made. 

Assume that in the previous example data will be analyzed in an independent *t*-test and the researcher was willing to collect 64 participants in each condition to achieve the 0.05 Type 1 error rate and 0.8 power. The researcher could have chosen to set the alpha level in this study to 0.1 instead of 0.05. If the Type 1 error rate is 0.1, the statistical power (given the same sample size of 64 per group) would be 0.88. The weighted combined error rate is now (0.5 × 0.1 + 0.5 × 0.12) = 0.11. In other words, increasing the Type 1 error rate from 0.05 to 0.1 reduced the Type 2 error rate from 0.2 to 0.12 and the combined error rate from 0.125 to 0.11. In the latter scenario, our total probability of making an erroneous decision has become 0.015 smaller. As shown below, this approach can be extended to incorporate scenarios where the prior probability of H0 and H1 differ. @mudge_setting_2012 and @kim2021choosing show that by choosing an alpha level based on the relative weight of Type 1 errors and Type 2 errors and assuming beliefs about the prior probability that H0 and H1 are correct, decisions can be made more efficiently than when the default alpha level of 0.05 is used. @kim2020decision \textcolor{black}{also provides an R-package to justify the alpha level based on decision-theoretic approaches, which provides solutions for a smaller set of power functions than the package accompanying this paper, and only allows users to minimize the costs of errors.} 

@winer_statistical_1962 writes: "The frequent use of the .05 and .01 levels of significance is a matter of convention having little scientific or logical basis. When the power of tests is likely to be low under these levels of significance, and when Type 1 and Type 2 errors are of approximately equal importance, the .30 and .20 levels of significance may be more appropriate than the .05 and .01 levels." The reasoning here is that a design that has 70% power for the smallest effect size of interest would not balance the Type 1 and Type 2 error rates in a sensible manner. Similarly, and perhaps more importantly, one should carefully reflect on the choice of the alpha level when an experiment achieves very high statistical power for all effect sizes that are considered meaningful. If a study has 99% power for effect sizes of interest, and thus a 1% Type 2 error rate, but uses the default 5% alpha level, it also suffers from a lack of balance. This latter scenario is quite common in meta-analyses, where researchers by default use a 0.05 alpha level, while the meta-analysis often has very high power for all effect sizes of interest. It is also increasingly common when analyzing large existing data sets or when collecting thousands of observations online. In such cases where power for all effects of interest is very high, it is sensible to lower the alpha level for statistical tests to reduce the weighted combined error rate and increase the severity of the test.

```{r compute1, include=FALSE}
res1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")
res1$alpha
res1$beta

res2 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance")
res2$alpha
res2$beta

res1_n_10 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")
res1_n_10$alpha
res1_n_10$beta

res1_n_100 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")
res1_n_100$alpha
res1_n_100$beta
```

Researchers can decide to either balance Type 1 and Type 2 error rates (e.g., designing a study such that the Type 1 and Type 2 error rate are equal) or minimize the weighted combined error rate. For any given sample size and effect size of interest there is an alpha level that minimizes the weighted combined Type 1 and Type 2 error rates. Because the chosen alpha level also influences the statistical power, and the Type 2 error rate is therefore dependent upon the Type 1 error rate, minimizing or balancing error rates requires an iterative optimization procedure.

As an example, imagine a researcher who plans to perform a study which will be analyzed with an independent two-sided *t*-test. They will collect 50 participants per condition, and set their smallest effect size of interest to Cohen's d = 0.5. They think a Type 1 error is just as costly as a Type 2 error, and believe H0 is just as likely to be true as H1. The weighted combined error rate is minimized when they set alpha to `r format(res1$alpha, digits = 3, nsmall = 2)` (see Figure \ref{fig:weight-plot}, dotted line), which will give the study a Type 2 error rate of beta = `r format(res1$beta, digits = 3, nsmall = 2)` to detect effects of d = 0.5. The weighted combined error rate is `r format((res1$beta + res1$alpha)/2, digits = 3, nsmall = 2)`, while it would have been `r format(((1-pwr::pwr.t.test(d = 0.5, n = 50, sig.level = 0.05, type = 'two.sample', alternative = 'two.sided')$power) + 0.05)/2, digits = 3, nsmall = 2)` if the alpha level was set at 5%^[For the same scenario, balanced error rates are alpha = `r format(res2$alpha, digits = 3, nsmall = 2)` and beta = `r format(res2$beta, digits = 3, nsmall = 2)`.]. 

We see that increasing the alpha level from the normative 5% level to `r format(res1$alpha, digits = 3, nsmall = 2)` reduced the weighted combined error rate - any larger or smaller alpha level would increase the weighted combined error rate. The reduction in the weighted combined error rate is not huge, but we have reduced the overall probability of making an error. More importantly, we have chosen an alpha level based on a justifiable principle, and clearly articulated the relative costs of a Type 1 and Type 2 error. Perhaps counter-intuitively, decision-making is sometimes slightly more efficient after *increasing* the alpha level from the default of 0.05 because a small increase in the Type 1 error rate can lead to a larger decrease in the Type 2 error rate. Had the sample size been much smaller, such as n = 10, the solid line in Figure \ref{fig:weight-plot} shows that the weighted combined error rate will always be high, but it is minimized if we increase the alpha level to alpha to `r format(res1_n_10$alpha, digits = 3, nsmall = 2)`. If the sample size had been n = 100, the optimal alpha level to minimize the weighted combined error rate (still assuming H0 and H1 have equal probabilities, and Type 1 and Type 2 errors are equally costly) is `r format(res1_n_100$alpha, digits = 3, nsmall = 2)` (the long-dashed line in Figure \ref{fig:weight-plot}). 

```{r, include = FALSE}

resplot1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = TRUE)
resplot2 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = TRUE)
resplot3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = TRUE)

plot_data <- rbind(resplot1$plot_data, resplot2$plot_data, resplot3$plot_data)
plot_data$n <- as.factor(rep(c(10, 50, 100), each = 9999))
```

```{r weight-plot, cahce=TRUE, fig.width=9, fig.height=7, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Weighted combined error rate (y-axis) for an independent *t*-test with n = 10, n = 50, and n = 100 per group and a smallest effect of interest of d = 0.5, for all possible alpha levels (x-axis)."}

w_c_alpha_plot <- ggplot(data=plot_data, aes(x=alpha_list, y=w_c_list)) +
  geom_line(size = 1.3, aes(linetype = n)) +
  geom_point(aes(x = resplot1$alpha, y = (1 * resplot1$alpha + 1 * (resplot1$beta)) / (1 + 1)), color="black", size = 3) +
  geom_point(aes(x = resplot2$alpha, y = (1 * resplot2$alpha + 1 * (resplot2$beta)) / (1 + 1)), color="black", size = 3) +
  geom_point(aes(x = resplot3$alpha, y = (1 * resplot3$alpha + 1 * (resplot3$beta)) / (1 + 1)), color="black", size = 3) +
  theme_minimal(base_size = 20) +
  theme(legend.key.size = grid::unit(5, "lines")) +
  scale_x_continuous("alpha", seq(0,1,0.1)) +
  scale_y_continuous("weighted combined error rate", seq(0,1,0.1), limits = c(0,1)) +
  theme(legend.position="bottom", legend.text=element_text(size=20))

w_c_alpha_plot
```

```{r, include=FALSE}
#Cohen would weigh Type 1 four times as much as Type 2 errors
#We set n to 50 participants and d = 0.5, and costT1T2 to 4.
#We indeed see the recommended alpha is 0.05 and beta is 0.2. 

res3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, printplot = TRUE)
res3$alpha
res3$beta
res3$errorrate

costT1T2 = 4
priorH1H0 = 1
w_c_3 <- (costT1T2 * res3$alpha + priorH1H0 * res3$beta) / (priorH1H0 + costT1T2)
w_c_3

res4 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 4, printplot = TRUE)
res4$alpha
res4$beta
res4$errorrate

costT1T2 = 4
priorH1H0 = 1
w_c_4 <- (costT1T2 * res4$alpha + priorH1H0 * res4$beta) / (priorH1H0 + costT1T2)

# If we weigh Type 1 errors 4 times more, we do not just add the two errors # and divide by 2, but we multiple Type 1 errors by 4 and divide by 5.
# We divide by 5 to keep the combined weighed error rate between 0 and 1.
# For example, if both errors are 1, we have (1 * 4 + 1)/5 = 1
# note this ignores differences in priors.
(res4$alpha * 4 + res4$beta)/(5)
# This is equivalent to:
(res4$alpha + res4$beta/4)/(1.25)
# This is just the formula above but filled in and ignoring priors.
# This is the value you see in the curve as weighed combined error rate.

# If the cost of an error is 10000, the total weighed cost is: 
w_c_4 * 10000

( (0.5 * 4 * res4$alpha * 10000) + (0.5 * res4$beta * 10000) ) 

# Replicate res4
a <- res4$alpha
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) ) 

# increasing alpha by 0.001 for analysis 4 increases costs 
a <- res4$alpha + 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) ) 

# decreasing alpha by 0.001 for analysis 4 increases costs 
a <- res4$alpha - 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) ) 

# Thus, this is the optimal alpha to minimize costs. 
```

## Weighing the Relative Cost of Errors

Cohen (1988) recommended a study design with a 5% Type 1 error rate and a 20% Type 2 error rate. He personally felt "Type I errors are of the order of four times as serious as Type II errors". However, some researchers have pointed out, following Neyman (1933), that false negatives might be more severe than false positives [@fiedler_long_2012]. The best way to determine the relative costs of Type 1 and Type 2 errors is by performing a cost-benefit analysis. For example, @field_minimizing_2004 quantify the relative costs of Type 1 errors when testing whether native species in Australia are declining. \textcolor{black}{In this example, the H1 is that the Koala population is declining and the H0 that the the Koala population is not declining. The Type 1 error would be to decide that the Koala population is declining, when in fact it is not; a Type 2 error would be to decide that the Koala population is not declining, when in fact it is.} @field_minimizing_2004 conclude that when it comes to the Koala population, given its great economic value, a cost-benefit analysis indicates the alpha level should be set to 1. In other words, one should always act as if the population is declining because the relative cost of a Type 2 error compared to a Type 1 error is too high.
\textcolor{black}{Note that in this example, the decision not to collect data is deterministically dominant} [@clemen_making_1997]. \textcolor{black}{The alpha of 1 shows that the results of the data collection will not influence future decisions in any way - it is always beneficial to intervene. This is arguably rare, but not incredibly rare. If you are bitten by an animal, it is possible to observe the animal for 10 days to see if it has rabies before you decide to go the the doctor for a rabies shot, but given the costs and benefits, it is more cost-efficient to assume the animal has rabies and get a rabies shot. In psychology, it is possible that accurate pilot studies to determine which of two possible manipulations has a larger effect size will require a larger sample than if one designs a study conservatively powered for the manipulation that based on a personal prior is believed to have the smallest effect size. There are similar situations where researchers might decide to skip a pilot study and immediately perform the main experiment because this is the most efficient choice.}

\textcolor{black}{An applied example where the decision is not deterministically dominant} can be found in @viamonte2006cost 
\textcolor{black}{who evaluate the benefits of a computerized intervention aimed at improving speed of processing to reduce car collisions in people aged 75 or older. They estimated that the risk of getting into an accident for these older drivers is 7.1\%. The cost of a collision was estimated to be \$22,000, or \$22,000 * 0.071 = 1,562.84 per driver in the USA. Furthermore, they estimate that the intervention can prevent accidents for 86\% of drivers. Therefore, the probability of a collision after intervention is now (1-0.86) * 0.071 = 0.00994. The total cost of completing the intervention was estimated to be \$274.50. When the intervention is implemented, some drivers will still get into a collision, so the total cost of the intervention and collisions is \$493.30 per driver (\$274.50 + 0.00994 * \$22,000).} 

\textcolor{black}{We can implement the intervention when it does not actually work, making a Type 1 error. The waste is \$274.50 per driver, as this is what the intervention costs even if it offers no benefits. If the intervention works, but it is not implemented, we make a Type 2 error and the amount of money that is not saved is \$1,562.84 (the cost of doing nothing) - \$493.30 (the cost if the intervention was implemented), for a waste of 1.069,54 per driver. This means that the relative cost of a Type 1 error compared to a Type 2 error is 274.50 /1.069,54 = 0.257, or the waste in money after a Type 1 error is 3.896 times (1.069,54/274.50) worse than a Type 2 error. This ratio reflects that the intervention is relatively cheap, and therefore a Type 1 error is not that costly, while the potential savings if collisions are prevented is relatively large. Of course, quantifying costs and benefits comes with uncertainties. The intervention might prevent more or less accidents, the risks of an accident for drivers of 75 years or older might be greater or smaller, etcetera. Sensitivity analyses can be used to compute a range of the ratio of the costs of Type 1 and Type 2 errors (see Viamonte et al., 2006). 
}

Although it can be difficult to formally quantify all relevant factors that influence the costs of Type 1 and Type 2 errors, there is no reason to let the perfect be the enemy of the good. In practice, even if researchers don't explicitly discuss their choice for the relative weight of Type 1 versus Type 2 errors, they make a choice in every hypothesis test they perform, even if they simply follow conventions (e.g., a 5% Type 1 error rate and a 20% Type 2 error rate). It might be especially difficult to decide upon the relative costs of Type 1 and Type 2 errors when there are no practical applications of the research findings, but even in these circumstances, it is up to the researcher to make a decision [@douglas_inductive_2000]. It is, therefore, worth reflecting on how researchers can start to think about the relative weight of Type 1 and Type 2 errors.

First, if a researcher only cares about not making a decision error, but the researcher does not care about whether this decision error is a false positive or a false negative, Type 1 and Type 2 errors are weighed equally. Therefore, weighing Type 1 and Type 2 errors equally is a defensible default, unless there are good arguments to weigh false positives more strongly than false negatives (or vice versa). When deciding upon whether there is a reason to weigh Type 1 and Type 2 errors differently, researchers are in essence performing a multiple criterion decision analysis [@edwards_advances_2007], and it is likely that treating the justification of the relative weight of Type 1 and Type 2 errors as a formal decision analysis would be a massive improvement over current research practices. A first step is to determine the objectives of the decision that is made in the hypothesis test, assign attributes to measure the degree to which these objectives are achieved within a specific time-frame [@clemen_making_1997], and finally to specify a value function. 
In a hypothesis test, we do not simply want to make accurate decisions, but we want to make accurate decisions given the resources we have available (e.g., time and money). Incorrect decisions have consequences, both for the researcher themselves, as for scientific peers, and sometimes for the general public. We know relatively little about the actual costs of publishing a Type 1 error for a researcher, but in many disciplines the costs of publishing a false claim are low, while the benefits of an additional publication on a resume are large.  However, by publishing too many claims that do not replicate, a researcher risks gaining a reputation for publishing unreliable work. In addition, a researcher might plan to build on work in the future, as might peers. The costs of experiments that follow up on a false lead might be much larger than the cost to reduce the possibility of a Type 1 error in an initial study, unless replication studies are cheap, will be performed anyway and will be shared with peers. However, it might also be true that the hypothesis has great potential for impact if true and the cost of a false negative might be substantial whenever it closes off a fruitful avenue for future research. A Type 2 error might be more costly than a Type 1 error, especially in a research field where all findings are published and people regularly perform replication studies to identify Type 1 errors in the literature [@fiedler_long_2012].

Another objective might be to influence policy, in which case the consequences of a Type 1 and Type 2 error should be weighed by examining the relative costs of implementing a policy that does not work against not implementing a policy that works. The second author once attended a presentation by a policy advisor who decided whether new therapies would be covered by the national healthcare system. She discussed Eye Movement Desensitization and Reprocessing (EMDR) therapy. She said that, although the evidence for EMDR was weak at best, the costs of the therapy (which can be done behind a computer) are very low, it was applied in settings where no good alternative therapies were available (e.g., inside prisons), and risk of negative side-effects was basically zero. They were aware of the fact that there was a very high probability that the claim that EMDR was beneficial might be a Type 1 error, but the cost of a Type 1 error was deemed much lower than the cost of a Type 2 error.

Imagine a researcher plans to collect 64 participants per condition to detect a d = 0.5 effect, and weighs the cost of Type 1 errors 4 times as much as Type 2 errors. To minimize error rates, the Type 1 error rate should be set to `r format(res4$alpha, digits = 3, nsmall = 2)`, which will make the Type 2 error rate `r format(res4$beta, digits = 3, nsmall = 2)`. If we would perform 20000 studies designed with these error rates, and assume H0 and H1 are equally likely to be true, we would observe 0.5 (the prior probability that H0 is true) × `r format(res4$alpha, digits = 3, nsmall = 2)` (the alpha level) × 20000 = `r round(0.5 * res4$alpha * 20000)` Type 1 errors, and 0.5 (the prior probability that H1 is true) × `r format(res4$beta, digits = 3, nsmall = 2)` (the Type 2 error rate) × 20000 = `r round(0.5 * res4$beta * 20000)` Type 2 errors. Since we weigh Type 1 errors 4 times as much as Type 2 errors, we multiple the cost of the `r round(0.5 * res4$alpha * 20000)` Type 1 errors by 4, which makes 4 × `r round(0.5 * res4$alpha * 20000)` = `r round(4 * 0.5 * res4$alpha * 20000)`, and to keep the weighted error rate between 0 and 1, we also multiply the 10000 studies where we expect H0 to be true by 4, such that the weighted combined error rate is (`r round(4 * 0.5 * res4$alpha * 20000)` + `r round(0.5 * res4$beta * 20000)`)/(40000 + 10000) = `r format((((4 * 0.5 * res4$alpha * 20000) + (0.5 * res4$beta * 20000))/(40000+10000)), digits = 3, nsmall = 2)`. Figure \ref{fig:cost-plot} visualizes the weighted combined error rate for this study design across the all possible alpha levels, and illustrated the weighted error rate is smallest when the alpha level is `r format(res4$alpha, digits = 3, nsmall = 2)`. 

```{r cost-plot, fig.width=9, fig.height=5, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Weighted combined error rate (y-axis) for an independent *t*-test with n = 64 per group and a smallest effect of interest of d = 0.5, where Type 1 errors are weighed 4 times as much as Type 2 errors, for all possible alpha levels (x-axis)."}

res4$plot

```

If the researcher had decided to *balance* error rates instead of *minimizing* error rates, we recognize that with 64 participants per condition, we are exactly in the scenario Cohen (1988) described. When Type 1 errors are considered 4 times as costly as Type 2 errors, 64 participants per condition yield a 5% Type 1 error rate and a 20% Type 2 error rate. If we would increase the sample size, The Type 1 and Type 2 error rates would remain in a balanced 1:4 ratio, but both error rates would be smaller. With a smaller sample size, both error rates would be larger.

## Incorporating Prior Probabilities

The choice for an optimal alpha level depends not just on the relative costs of Type 1 and Type 2 errors, but also on the base rate of true effects [@miller_quest_2019]. \textcolor{black}{In the extreme case, in all studies a researcher designs H1 is true.} In this case, there is no reason to worry about Type 1 errors, because a Type 1 error can only happen when the null hypothesis is true. Therefore, you can set the alpha level to 1 without any negative consequences. 
\textcolor{black}{On the other hand, if the base rate of true H1s is very low, you are more likely to test a hypothesis where H0 is true. Therefore, the probability of observing a false positive becomes a more important consideration.}
 Whatever the prior probabilities are believed to be, researchers always need to specify the prior probabilities of H0 and H1. Researchers should take their expectations about the probability that H0 and H1 are true into account when evaluating costs and benefits.

For example, let's assume a researcher performs 1000 studies. The researcher expects 100 studies to test a hypothesis where H1 is true, while the remaining 900 studies test a hypothesis where H0 is true. This means H0 is believed to be 9 times more likely than H1, or equivalently, that the relative probability of H1 versus H0 is 0.1111:1. However, the researcher decides to ignore these prior probabilities and designs a study that has the normative 5% Type 1 error rate and a 20% Type 2 error rate. The researcher should expect to observe 0.9 (the prior probability that H0 is true) × 0.05 (the alpha level) × 1000 = `r 0.9 * 0.05 * 1000` Type 1 errors, and 0.1 (the prior probability that H1 is true) × 0.2 (the Type 2 error rate) × 1000 = `r 0.1 * 0.2 * 1000` Type 2 errors, for a total of `r (0.9 * 0.05 * 1000) + (0.1 * 0.2 * 1000)` errors. 

However, the total number of errors does not tell the whole story, as Type 1 errors are weighed four times more than Type 2 errors. We therefore need to compute the weighted combined error rates *w* taking the relative cost of Type 1 and Type 2 errors into account, and the prior probabilities of H0 and H1, which can be done with the following formula from @mudge_setting_2012: 

\begin{equation}
\frac{(cost_{T1T2} \times \alpha + prior_{H1H0} \times \beta)}{prior_{H1H0}+cost_{T1T2}}
\label{eq:minimize}
\end{equation}

```{r, include=FALSE}
res5 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, priorH1H0 = 0.11111)
res5$alpha
res5$beta
res5$error

res6 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 4, priorH1H0 = 0.11111, printplot = TRUE)
res6$alpha
res6$beta
res6$errorrate

costT1T2 = 1
priorH1H0 = 2
w <- (costT1T2 * res6$alpha + priorH1H0 * (res6$beta)) / (priorH1H0 + costT1T2)

```

For the previous example, the weighted combined error rate is (4 × 0.05 + 0.1111 × 0.2) / (0.1111 + 4) = 0.054. If the researcher had taken the prior probabilities into account when deciding upon the error rates, a lower combined error rate can be achieved. With the same sample size (64 per condition) the combined weighted error rate was not as small as possible, optimally balanced error rates (maintaining the 4:1 ratio of the weight of Type 1 versus Type 2 errors) would require setting alpha to `r format(res5$alpha, digits = 3, nsmall = 2)` and the Type 2 error rate to `r format(res5$beta, digits = 3, nsmall = 2)`. The researcher should now expect to observe 0.9 (the prior probability that H0 is true) × `r format(res5$alpha, digits = 3, nsmall = 2)` (the alpha level) × 1000 = `r 0.9 * res5$alpha * 1000` Type 1 errors, and 0.1 (the prior probability that H1 is true) × `r format(res5$beta, digits = 3, nsmall = 2)` (the Type 2 error rate) × 1000 = `r 0.1 * res5$beta * 1000` Type 2 errors. The weighted error rate is `r format(res5$errorrate, digits = 3, nsmall = 2)`.

Because the prior probability of H0 and H1 influence the expected number of Type 1 and Type 2 errors one will observe in the long run, the alpha level should be lowered as the prior probability of H0 increases, or equivalently, the alpha level should be increased as the prior probability of H1 increases. Because the base rate of true hypotheses is unknown, this step requires a subjective judgment. This can not be avoided, because one always makes assumptions about base rates, even if the assumption is that a hypothesis is equally likely to be true as false (with both H1 and H0 having a 50% probability). In the previous example, it would also have been possible minimize (instead of balance) the error rates, which is achieved with an alpha of `r format(res6$alpha, digits = 3, nsmall = 2)` and a beta of `r format(res6$beta, digits = 3, nsmall = 2)`, for a total of `r (0.9 * res6$alpha * 1000) + (0.1 * res6$beta * 1000)` errors, where the weighted error rate is `r format(res6$errorrate, digits = 3, nsmall = 2)`.

The two approaches (balancing error rates or minimizing error rates) typically yield quite similar results. Where minimizing error rates might be slightly more efficient, balancing error rates might be slightly more intuitive (especially when the prior probability of H0 and H1 is equal). Note that although there is always an optimal choice of the alpha level, there is always a range of values for the alpha level that yield quite similar weighted error rates, as can be seen in Figure \ref{fig:cost-plot}. 

## Increasing the Alpha Level Above 0.05

\textcolor{black}{Many empirical sciences have recently been troubled by a replication crisis}
[@camerer_2016; @open2015estimating], \textcolor{black}{which has in part been caused by inflated alpha levels due to $p$-hacking} [@simmons2011false], \textcolor{black}{publication bias, and low statistical power} [@lindsay_2015]. \textcolor{black}{In light of this low replicability, a potential concern about allowing researchers to justify their alpha level is that researchers can decide to increase the alpha level above the 0.05 threshold. This could increase the rate of false positives published in the literature compared to when an alpha level of 0.05 remains the norm. An increase of the alpha level should only be deemed acceptable when authors can justify that the costs of the increase in the Type 1 error rate is sufficiently compensated by the benefit of decreased Type 2 error rate. Furthermore, researchers should explicitly accompany claims by their error rates throughout an article, especially when the alpha level is increased, and readers of claims made with higher alpha level should understand such claims are made with greater uncertainty, and could very well be false.} 

\textcolor{black}{There are circumstances under which optimal error rates will require an increase of the alpha level, which will also increase the number of false positives in the literature. Assuming the goal of scientists is to efficiently generate reliable knowledge, the proposal to increase the alpha level (and thus to increase the Type 1 error rate in the literature) should only be adopted if the cost of an increase in Type 1 errors is compensated in some way. So far we have focussed only on how the increase in the Type 1 error rate will lead to a greater reduction in the Type 2 error rate, which all else being equal, should improve decision making in hypothesis tests. In practice, it might be a challenge to reach agreement on the weight of Type 1 and Type 2 errors among different stakeholders. For example, where a team of researchers might believe a Type 1 and Type 2 error is equally costly, an editor of a journal might weigh Type 1 errors more than Type 2 errors. We should also consider the possibility that researchers try to opportunistically specify the relative cost of Type 1 and Type 2 error rates to increase their alpha level, and increase the probability of finding a 'significant' effect.}

\textcolor{black}{Nevertheless, in some cases, it can be justified to increase the alpha level above the 0.05 threshold. These will usually be cases where (1) the study will have directly decision-making relevant implications (as in the above EDM example), (2) a cost-benefit analysis is provided that gives a clear rationale for relatively high costs of a Type 2 error, (3) the probability of H1 being false is relatively low, and (4) it is not feasible to reduce overall error rates by collecting more data. In these cases, it will often be desirable to justify the alpha level during the first phase of a Registered Report so that the higher alpha level that will be used in a study can be discussed transparently during peer-review. At the same time, given the complexity of weighing the costs and benefits of research, it is understandable if some journals consider such discussions too great a burden for reviewers. If so, these journals could indicate that they limit deviations from an alpha level of 0.05 only where researchers increase the severity of their test by lowering the alpha level.} 

\textcolor{black}{Journals might also prefer to use a default alpha level of 0.05 to reduce the burden on readers to examine at which alpha level claims in their journal are made. Especially if an increase in alpha levels was not evaluated by peers during the first phase of a Registered Report, the evaluation of whether this alpha level was appropriate is left to readers. In practice, the use of a higher alpha level will require readers to keep track of the fact that the claim of the presence of an effect was less severely tested than it would have been with a default alpha, instead of keeping track of the fact that claims of the absence of an effect were less severely tested than they would have been when the statistical power had been higher (i.e., by increasing the alpha level). In a science where people only focus on significant effects and treat all significant effects as equally well supported, increasing alpha levels could lead to a sense of false certainty about a body of work. If the practice to increase alpha levels becomes popular, it will be important to examine whether varying alpha levels are taken into account when interpreting and discussing research findings, and how negative side-effects can be mitigated.} 

\textcolor{black}{Finally, the use of a high alpha level might be missed if readers skim an article. We believe this can be avoided by having each scientific claim accompanied by the alpha level under which it was made. Scientists should be required to report their alpha levels prominently, usually in the abstract of a paper alongside a summary of the main claim. The correct interpretation of a hypothesis test was never to label an effect as 'significant' or 'nonsignificant' but to reject effects implied by the null model with a specific error rate. Replacing 'the effect was significant' with 'we reject an effect size of 0 with a 10\% error rate' might end up improving the interpretation of hypothesis tests. Note that by explicitly reporting the alpha level alongside a claim it will also become more visible when researchers lower their alpha level, and this practice will therefore clearly communicate whenever readers should be impressed by the fact that a claim passed an even more severe test than if a traditional alpha level of 0.05 would have been used.}

## Sample Size Justification when Minimizing or Balancing Error Rates

So far we have illustrated how to perform what is known as a *compromise power analysis* where the weighted combined error rate is computed as a function of the sample size, the effect size, and the desired ratio of Type 1 and Type 2 errors [@erdfelder_gpower_1996]. However, in practice researchers will often want to justify their sample size based on an *a-priori power analysis* where the required sample size is computed to achieve desired error rates, given an effect size of interest [@lakens_sample_2021]. It is possible to determine the sample size at which we achieve a certain desired weighted combined error rate. This requires researchers to specify the effect size of interest, the relative cost of Type 1 and Type 2 errors, the prior probabilities of H0 and H1, whether error rates should be balanced or minimized, and the desired weighted combined error rate. 

```{r, include = FALSE}
res7 <- optimal_sample(power_function = "pwr::pwr.t.test(d = 0.5, n = sample_n, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")

# # Result is identical to:
# pow_res <- ceiling(pwr::pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.95, type = 'two.sample', alternative = 'two.sided')$n)


# res8 <- optimal_sample(power_function = "pwr::pwr.t.test(d = 0.5, n = i, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", errorgoal = 0.05, error = "minimize", costT1T2 = 1, priorH1H0 = 0.1)
# 
# restest <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 65, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 1, priorH1H0 = 0.1)
# 
#  restest2 <- optimal_sample(power_function = "pwr::pwr.t.test(d = 0.5, n = sample_n, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", errorgoal = 0.05, error = "balance", costT1T2 = 1, priorH1H0 = 1)

```

Imagine a researcher is interested in detecting an effect of Cohen's d = 0.5 with a two-sample *t*-test. The researcher believes Type 1 errors are equally costly as Type 2 errors and believes a H0 is equally likely to be true as H1. The researcher desires a minimized weighted combined error rate of 5%. Figure \ref{fig:error-plot} shows the optimal alpha level, beta, and weighed combined error rate as a function of sample size for this situation. We can optimize the weighted combined error rate as a function of the alpha level and sample size through an iterative procedure, which reveals that a sample size of `r res7$samplesize` participants in each independent condition is required to achieve the desired weighted combined error rate. In the specific cases where the prior probability of H0 and H1 are equal, this sample size can also be computed directly with common power analysis software by entering the desired alpha level and statistical power. In this example, where Type 1 and Type 2 error rates are weighted equally, and the prior probability of H0 and H1 is assumed to be 0.5, the sample size is identical to that required to achieve an alpha of 0.05 and a desired statistical power for d = 0.5 of 0.95. 
```{r, include=FALSE}
# alphas <- c()
# betas <-c()
# Error <-c()
# 
# power_function = "pwr::pwr.t.test(d = 0.5, n = i, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power"
# for (i in 10:150) {
# res <- optimal_alpha(power_function = paste(stringr::str_replace(power_function, "n = i", paste("n  =", i))), costT1T2 = 1)
# alphas <- c(alphas, res$alpha)
# betas <- c(betas, res$beta)
# Error <- c(Error, res$errorrate)
# }
# 
# data <- as.data.frame(cbind(alphas, betas, Error))
# write.csv(data, "plotdata.csv")
```


```{r, include = FALSE}
data <- read.csv("plotdata.csv")
samplesize <- 10:150
line <- rep(0.05, length(samplesize))
data <- as.data.frame(cbind(samplesize,line, data))

plot <- ggplot(aes(x=samplesize), data=data) +
    geom_line(aes(y = Error), size = 1.3, linetype = "solid", color="black") +
    theme_minimal(base_size = 20) +
    scale_x_continuous("Sample Size", seq(10,150,20)) +
    scale_y_continuous("",seq(0,1,0.05), limits = c(0,0.5)) + 
    geom_line(aes(y=alphas), data=data, size = 1.3, linetype = "dashed", color="grey60") + 
    geom_line(aes(y=betas), data=data, size = 1.3, linetype = "dotted", color="grey60") +  
    geom_point(aes(x = samplesize[Error == max(Error[Error < 0.05])], y = 0.05), size = 4, color = "red") +
    geom_segment(aes(x = samplesize[Error == max(Error[Error < 0.05])], xend= samplesize[Error == max(Error[Error < 0.05])], y = 0.05, yend = -Inf), color = "black", linetype = "solid", size = 1.3) +
    labs(x = "Sample Size", y = "")
  
```

```{r error-plot, fig.width=10, fig.height=6, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Weighted combined error rate (solid black line), alpha (lower grey dashed line), and beta (upper grey dotted line) for an independent *t*-test as a function of sample size when the alpha level is justified based on the goal to minimize the error rate at each sample size. The sample size corresponding to the black dot is the minimum required sample size to achieve a 5% weighted combined error rate."}
plot
```

## Lowering the Alpha Level to Avoid Lindley's Paradox

Formally controlling the costs of errors can be a challenge, as it requires researchers to specify the relative cost of Type 1 and Type 2 errors, prior probabilities, and the effect size of interest. Due to this complexity, researchers might be tempted to fall back on the heuristic use of an alpha level of 0.05. Fisher [-@fisher_design_1971] referred to the default alpha level of 0.05 as a “convenient convention” and one can argue suffices as a low enough threshold to make scientific claims in a scientific system where we have limited resources and value independent replications [@tunc_epistemic_2021]. 

However, there is a well-known limitation of using a fixed alpha level that has lead statisticians to recommend choosing an alpha level as a function of the sample size. 
\textcolor{black}{This was suggestion of a flexible decision criterion was already mentioned by the statistician Harold Jeffreys in a letter he wrote to Fisher in 1934} [@WagenmakersJeffreys]. \textcolor{black}{Jeffreys later stated more explicitly that the critical value should increase with the sample size: "The results show that the probability that such a term is needed is increased or decreased according as the coefficient is more or less than a certain multiple of its standard error; the multiple needed, however, increases with the number of observations.”} [@jeffreys1936on].

To understand the argument behind this recommendation, it is important to distinguish between statistical inferences based on error control and inferences based on likelihoods. An alpha level of 5% will limit incorrect decisions to a desired maximum (in the long run, and when all test assumptions are met). However, from a likelihood perspective it is possible that the observed data is much more likely when the null hypothesis is true than when the alternative hypothesis is true, even when the observed *p*-value is smaller than 0.05. This situation, known as Lindley's paradox, is visualized in Figure \ref{fig:p-plot}. 

To prevent situations where a frequentist rejects the null hypothesis based on *p* < 0.05, when the evidence in the test favors the null hypothesis over the alternative hypothesis, it is recommended to lower the alpha level as a function of the sample size. The need to do so is discussed extensively by @leamer_specification_1978. He writes "The rule of thumb quite popular now, that is, setting the significance level arbitrarily to .05, is shown to be deficient in the sense that from every reasonable viewpoint the significance level should be a decreasing function of sample size." The same point was already recognized by @Jeffreys1939, who discusses ways to set the alpha level in the Neyman-Pearson approach to statistics: "We should therefore get the best result, with any distribution of alpha, by some form that makes the ratio of the critical value to the standard error increase with n. It appears then that whatever the distribution may be, the use of a fixed *P* limit cannot be the one that will make the smallest number of mistakes." Similarly, @good_bayes-non-bayes_1992 notes: "we have empirical evidence that sensible *P* values are related to weights of evidence and, therefore, that *P* values are not entirely without merit. The real objection to *P* values is not that they usually are utter nonsense, but rather that they can be highly misleading, especially if the value of N is not also taken into account and is large."

Lindley's paradox emerges because in frequentist statistics the critical value of a test approaches a limit as the sample size increases (e.g., *t* = 1.96 for a two-sided *t*-test with an alpha level of 0.05). It does not emerge in Bayesian hypothesis tests because the inference criterium requires a larger test statistic as the sample size increases [@rouder_bayesian_2009; @zellner_introduction_1971]. \textcolor{black}{One possible inference criterium in Bayesian statistics is the Bayes factor} [@kass1995bayes].  
\textcolor{black}{A Bayes factor contrasts the probability of the data under the competing hypotheses considered. When comparing H1 to H0 it is given by Equation \ref{eq:bf}.} 
\begin{equation}
\frac{p(data|H_1)}{p(data|H_0)}
\label{eq:bf}
\end{equation}

\textcolor{black}{Note that the equation shows a crucial difference between $p$-values and Bayes factors: A $p$-value depends only on the probability of the data or more extreme data under H0, whereas the Bayes factor takes both H0 and H1 into account.}

\textcolor{black}{A Bayes factor of 1 implies equal evidence for H0 and H1. Although any discretization inevitably results in loss of information, as a rule of thumb, Bayes factors between 3 and 10 imply moderate evidence for H1 and Bayes factors larger 10 strong evidence} [@Jeffreys1939; @LeeWagenmakersBayesBook]. To prevent Lindley's paradox when using frequentist statistics one would need to adjust the alpha level in a way that the likelihood ratio (also called the Bayes factor) at the critical test statistic is not larger than 1. With such an alpha level, a significant *p*-value will always be at least as likely if H1 is true than if H0 is true, which avoids Lindley's paradox. @rouder_bayesian_2009 and @faulkenberry2019estimating developed Bayes factors for *t*-tests and Analysis of Variance (ANOVA) which can calculate the Bayes factor from the test statistic and degrees of freedom. We developed a Shiny app that lowers the alpha level for a *t*-test or ANOVA, such that the critical value that leads researchers to reject H0 is also high enough to guarantee (under the assumption of the priors) that the data provide relative evidence in favor of H1.

There are two decisions that should be made when desiring to prevent Lindley's paradox, the first about the prior, and the second about the threshold for the desired evidence in favor of H1. Both @leamer_specification_1978 and @good_bayes-non-bayes_1992 offer their own suggestions. We rely on a unit information prior for the ANOVA and a Cauchy prior with scale 0.707 for *t*-tests (although the package allows users to adjust the r scale). Both of these priors are relatively wide, which makes them a conservative choice when attempting to prevent the Lindley's paradox. The choice for this prior is itself a 'convenient convention', but the approach extends to other priors researchers prefer, and researchers can write custom code if they want to specify a different prior. A benefit of the chosen defaults for the priors is that, in contrast to previous approaches that aimed to calculate a Bayes factor for every *p*-value [@colquhoun_false_2019; @colquhoun_reproducibility_2017], researchers do not need to specify the effect size under the alternative hypothesis. This lowers the barrier of adopting this approach in situations where it is difficult to specify a smallest effect size of interest or an expected effect size.

A second decision is the threshold of the Bayes factor used to lower the alpha level. Using a Bayes factor of 1 formally prevents Lindley's paradox. It does mean that one might reject the null hypothesis when the data provide just as much evidence for H1 as for H0. Although it is important to note that researchers will often observe *p*-values well below the critical value, and thus, in practice the evidence in the data will be in favor of H1 when H0 is rejected, researchers might want to increase the threshold of the Bayes factor that is used to lower the alpha level to prevent weak evidence [@Jeffreys1939]. This can be achieved by setting the threshold to a larger value than 1 (e.g., BF > 3). The Shiny app allows researchers to adjust the alpha level in a way that a significant *p*-value will always provide moderate (BF > 3) or strong (BF > 10) evidence against the null hypothesis.

```{r, Lindley150, fig.width=8, fig.height=6, fig.cap = "Relationship between $p$-value and Bayes factor for a one-sample $t$-test with 150 participants using a Cauchy prior."}

n1 <- 150

loops <- seq(from = 0, to = 7, by = 0.01)
p <- numeric(length(loops))
bf <- numeric(length(loops))
#d <- numeric(length(loops))
tval <- numeric(length(loops))
i <- 0

for(t in loops){
  i <- i+1
  bf[i] <- exp(BayesFactor::ttest.tstat(t, n1, rscale = 0.707, nullInterval = c(0, Inf))$bf)
  p[i] <- pt(t, ((n1) - 1), lower=FALSE)
  tval[i] <- t
  #d[i] <- t * sqrt((1/n1)+(1/n2))
}

lindley  <- ttestEvidence(1,  150, one.sided = TRUE, rscale = 0.707)[[1]]
moderate <- ttestEvidence(3,  150, one.sided = TRUE, rscale = 0.707)[[1]]
strong   <- ttestEvidence(10, 150, one.sided = TRUE, rscale = 0.707)[[1]]

plot(p, bf, type="l", lty=1, lwd=3, xlim = c(0, 0.05), ylim = c(0, 10), axes = F, xlab = "p-value", ylab = "Bayes factor", cex.lab = 1.5, cex.main = 1.5, cex.sub = 1.5)
axis(side=1, at = c(0, as.numeric(lindley), as.numeric(moderate), as.numeric(strong), 0.05), labels = c(0, round(lindley, digits = 3), round(moderate, digits = 3), round(strong, digits = 3), 0.05),  lwd = 3, las = 3)
axis(side=2, at = c(0.33, 1, 3, 10), labels = c("1/3", 1, 3, 10), lwd = 3, las = 2)

abline(h = c(1, 3, 10), col = "gray25", lty = 2, lwd = 1.5)
abline(v = c(lindley, moderate, strong), lty = 3, lwd = 1.5)
```

To illustrate this approach to justifying the alpha level as a function of the sample size, imagine a researcher collected 150 observations in a within-subjects design where they aim to test a directional prediction in a dependent $t$-test. For any sample size and choice of prior, a *p*-value is directly related to a Bayes factor. Figure \ref{fig:Lindley150} shows the relationship of two-sided *p*-values and Bayes factors using a Cauchy prior with a r-scale of 0.707 given a sample size of 150 for a within-subjects $t$-test. To avoid Lindley's paradox, the researcher would need to use an alpha level of `r printnum(lindley, digits = 4)` for the one-sided $t$-test, given the chosen prior, as this choice for an alpha level guarantees that a significant *p*-value will correspond to evidence in favor of H1.

```{r, include = FALSE}
ppenny <- ftestEvidence(1, 1, 798)
```

\textcolor{black}{To give a practical example of how the alpha level can be justified to prevent Lindley's paradox, we can re-examine a study by} @pennycook2019lazy 
\textcolor{black}{who investigated sharing of misinformation on social media. They report that Clinton supporters were better able to discern fake news from real news than Trump supporters, $F$(1, 798) = 28.95, $p$ < .001. However, given the large number of observations, which likely provide very high power for all effect sizes that would be considered large enough to be meaningful, one could have decided to reduce the alpha level so that any observed significant $p$-value can also be interpreted as evidence for the alternative hypothesis. If the authors had justified their alpha level as a function of their sample size as described above, they would have set the alpha level to} `r printnum(ppenny$alpha, digits = 3)`. 
\textcolor{black}{Calculating the precise $p$-value of 9.77 x $10^{-8}$ shows their result is still significant using this more stringent alpha level.} @pennycook2019lazy \textcolor{black}{could have designed a study where the choice of the alpha level would have prevented significant results from being evidence for the null hypothesis. Note that by choosing an alpha level that prevents Lindley’s paradox, the study would also have more balanced error rates (Wagenmakers \& Ly, 2021), thereby improving optimal decision making. By lowering the alpha level at the expense of a relatively modest drop in statistical power, the authors would have more severely tested their hypothesis. Given the observed $p$-value, the study would have provided even more impressive support for their prediction due to the smaller Type 1 error rate.}

For small sample sizes it is possible to guarantee that a significant result is evidence for the alternative hypothesis using an alpha level that is higher than 0.05. It is not recommended to use the procedure outlined in this section to *increase* the alpha level above the conventional choice of an alpha level (e.g., 0.05). This approach to the justification of an alpha level assumes researchers first want to control the error rate, and as a secondary aim want to prevent Lindley's paradox by reducing the alpha level as a function of the sample size where needed. Figure \ref{fig:lindleyplot} shows the alpha levels for different values of N for between and within subjects $t$-test. We can see that particularly for within-subjects $t$-tests the alpha level rapidly falls below 5% as the sample size increases.


```{r, include = FALSE}
# samplesize    <- seq(30, 300, 2)
# cauchywithin  <- c()
# cauchybetween <- c()
# cauchywithinone    <- c()
# cauchybetweenone   <- c()
# 
# for (i in samplesize){
#  cauchywithin   <- c(cauchywithin,     ttestEvidence(1, i))
#  cauchywithinone     <- c(cauchywithinone,       ttestEvidence(1, i, one.sided = T))
#  cauchybetween  <- c(cauchybetween,    ttestEvidence(1, i/2, i/2))
#  cauchybetweenone    <- c(cauchybetweenone,       ttestEvidence(1, i/2, i/2, one.sided = T))
# }
# 
#  data <- as.data.frame(cbind(samplesize, cauchywithin, cauchybetween, unitwithin, unitbetween))
#  write.csv(data, "plotlindleydata.csv")
```

```{r, include = FALSE}
data <- read.csv("plotlindleydata.csv")

plot <- ggplot(aes(x=samplesize), data=data) +
    geom_line(size = 1.3, aes(y = cauchywithin), color = "grey60", linetype = "dashed") +
    geom_line(size = 1.3, aes(y = cauchybetween), color = "black", linetype = "solid") +
    theme_minimal(base_size = 20) +
    scale_x_continuous("Total Sample Size (N)", seq(10,300,20)) +
    scale_y_continuous("Alpha", seq(0,0.125,0.025), limits = c(0, NA)) +
  theme(legend.position="bottom", legend.text=element_text(size=20)) + 
  theme(legend.key.size = grid::unit(5, "lines")) 
    
```

```{r lindleyplot, fig.width=10, fig.height=6, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Optimal alpha level for within (grey dashed line) and between-sample (solid black line) two-sided $t$-tests."}
plot
```

# When to Minimize Alpha Levels and When to Avoid Lindley's Paradox

\textcolor{black}{When should we minimize or balance error rates and when should we avoid Lindley's paradox? In practice, it might be most convenient to minimize or balance error rates whenever there is enough information to conduct a power analysis, and if researchers feel comfortable specifying the relative cost of Type 1 and Type 2 errors, and have a decent empirically justified estimate of prior probabilities of the null and alternative hypothesis. This is more likely for applied research, as in the case of the test of an intervention for older drivers discussed previously. When a study has direct policy implications the costs of Type 1 error (the policy being implemented although it does not work) in comparison to a Type 2 error (the policy is not implemented even though it does work) can often be assessed by means of cost-benefit analysis. It is important to note that the approach which tries to minimize or balance error rates will in practice also reduce the alpha level as a function of sample size and should therefore avoid Lindley’s paradox in most applied cases (although it does not guarantee to do so). If researchers do not feel they can specify these parameters, they can fall back on the approach to lower the alpha level as a function of the sample size to prevent Lindley’s paradox. This might often be the more feasible approach in basic research.} 

\textcolor{black}{In addition, the two approaches differ with regard to their underlying philosophy of science. The first is based on decision theoretical developments that build on a Neyman-Pearson approach and might, therefore, be more attractive to researchers whose inferential philosophy is based on statistical decision-theory. The second approach, on the other hand, offers a Bayes-Non-Bayes hybrid combining frequentist and Bayesian statistics, which might be more attractive to researchers who care about both statistical schools} [@good_bayes-non-bayes_1992].


# Discussion

As the choice of error rates is an important decision in any hypothesis test, authors should always be expected to justify their choice of error rates whenever they use data to make decisions about the presence or absence of an effect. As @skipper_sacredness_1967 remark: ``If, in contrast with present policy, it were conventional that editorial readers for professional journals routinely asked: What justification is there for this level of significance? authors might be less likely to indiscriminately select an alpha level from the field of popular eligibles.'' It should especially become more common to lower the alpha level when analyzing large data sets or when performing meta-analyses, whenever each test has very high power to detect any effect of interest. Researchers should also consider increasing the alpha level when the combination of the effect size of interest, the sample size, the relative cost of Type 1 and Type 2 errors, and the prior probability of H1 and H0 mean this will improve the efficiency of decisions that are made. 

A Shiny app is available that allows users to perform the calculations recommended in this article. It can be used to minimize or balance alpha and beta by specifying the effect size of interest and the sample size, as well as an analytic power function. The effect size should be determined as in a normal a-priori power analysis (preferably based on the smallest effect size of interest, for recommendations, see @lakens_sample_2021). Alternatively, researchers can lower the alpha level as a function of the sample size by specifying only their sample size. In a Neyman-Pearson approach to statistics the alpha level should be set before the data is collected. Whichever approach is used, it is strongly recommended to preregister the alpha level that researchers plan to use before the data is collected. In this preregistration, researchers should document and explain all assumptions underlying their decision for an alpha level, such as beliefs about prior probabilities or choices for the relative weight of Type 1 and Type 2 errors.

\textcolor{black}{In this paper, we presented two ways of justifying alpha levels, the first based on minimizing or balancing the relative costs of errors, and the second based on avoiding Lindley's paradox. Additional approaches to justifying the alpha level have been presented, such as}
@bayarri2016rejection 
\textcolor{black}{, who propose to justify the alpha level based on the strength of evidence (1-beta)/alpha. We look forward to the development of additional approaches, and hope that in the future researchers will have multiple tools in their statistical toolbox to justify alpha levels.}

Throughout this manuscript we have reported error rates rounded to three decimal places. Although we can compute error rates to many decimals, it is useful to remember that the error rate is a long run frequency, and in any finite number of tests (e.g., all the tests you will perform in your lifetime) the observed error rate varies somewhere around the long run error rate. The weighted combined error rate might be quite similar across a range of alpha levels, or when using different justifications (e.g., or balancing versus minimizing alpha levels in a cost-benefit approach) and small differences between alpha levels might not be noticeable in a limited number of studies in practice. We recommend preregistering alpha levels up to three decimals, while keeping in mind there is some false precision in error rates with too many decimals.  

Because of the strong norms to use a 5% error rate when designing studies, there are relatively few examples of researchers who attempt to justify the use of a different alpha level. Within specific research lines researchers will need to start to develop best practices to decide how to weigh the relative cost of Type 1 and Type 2 errors, or quantify beliefs about prior probabilities. It might be a challenge to get started, but the two approaches illustrated here provide one way to move beyond the mindless use of a 5% alpha level, and make more informative decisions when we test hypotheses.

# Funding

This work was funded by VIDI Grant 452-17-013 from the Netherlands Organisation for Scientific Research.

# Supplemental material

All code used to create this manuscript is provided at \url{https://github.com/Lakens/justify_alpha_in_practice}. Information about the JustifyAlpha R package and Shiny app is available at \url{https://lakens.github.io/JustifyAlpha/index.html}.
  
# Prior versions

A preprint of this article is available at \url{https://doi.org/10.31234/osf.io/ts4r6}.

\newpage
# References

\begingroup
\interlinepenalty = 10000 
<div id="refs" custom-style="Bibliography"></div>
\endgroup

@article{albers_when_2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis}
}

@article{bakan_test_1966,
  title = {The Test of Significance in Psychological Research.},
  author = {Bakan, David},
  year = {1966},
  volume = {66},
  pages = {423--437},
  journal = {Psychological bulletin},
  number = {6}
}

@article{bem_feeling_2011,
  title = {Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl J.},
  year = {2011},
  month = mar,
  volume = {100},
  pages = {407--425},
  issn = {1939-1315},
  doi = {10.1037/a0021524},
  abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by "time-reversing" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.},
  journal = {Journal of Personality and Social Psychology},
  keywords = {Affect,Awareness,Boredom,Cognition,Erotica,Escape Reaction,Female,Habituation; Psychophysiologic,Humans,Male,Mental Recall,Parapsychology,Subliminal Stimulation,Time Factors},
  language = {eng},
  number = {3},
  pmid = {21280961}
}

@article{benjamin_redefine_2018,
  title = {Redefine Statistical Significance},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and Boeck, Paul De and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Ho, Teck Hua and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Zandt, Trisha Van and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  year = {2018},
  month = jan,
  volume = {2},
  pages = {6--10},
  issn = {2397-3374},
  doi = {10.1038/S41562-017-0189-Z},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  annotation = {00170},
  copyright = {2017 The Author(s)},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {1}
}

@article{berger_testing_1987,
  title = {Testing a Point Null Hypothesis: The Irreconcilability of {{P}} Values and Evidence},
  shorttitle = {Testing a Point Null Hypothesis},
  author = {Berger, James O. and Sellke, Thomas},
  year = {1987},
  volume = {82},
  pages = {112--122},
  journal = {Journal of the American statistical Association},
  number = {397}
}

@article{berger_unified_1997,
  title = {Unified Frequentist and {{Bayesian}} Testing of a Precise Hypothesis},
  author = {Berger, James O. and Boukai, Ben and Wang, Yinping and others},
  year = {1997},
  volume = {12},
  pages = {133--160},
  journal = {Statistical Science},
  number = {3}
}

@book{cohen_statistical_1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed},
  publisher = {{L. Erlbaum Associates}},
  address = {{Hillsdale, N.J}},
  isbn = {978-0-8058-0283-2},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  lccn = {HA29 .C66 1988}
}

@article{colquhoun_false_2019,
  title = {The {{False Positive Risk}}: {{A Proposal Concerning What}} to {{Do About}} p-{{Values}}},
  shorttitle = {The {{False Positive Risk}}},
  author = {Colquhoun, David},
  year = {2019},
  month = mar,
  volume = {73},
  pages = {192--201},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1529622},
  abstract = {It is widely acknowledged that the biomedical literature suffers from a surfeit of false positive results. Part of the reason for this is the persistence of the myth that observation of p {$<$} 0.05 is sufficient justification to claim that you have made a discovery. It is hopeless to expect users to change their reliance on p-values unless they are offered an alternative way of judging the reliability of their conclusions. If the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. One such proposal is based on calculation of false positive risk(FPR). It is suggested that p-values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p-value. This number could be the minimum FPR (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). Alternatively one could specify the prior probability that it would be necessary to believe in order to achieve an FPR of, say, 0.05.},
  journal = {The American Statistician},
  keywords = {Bayes,False positive,False positive report probability,False positive risk,FPR,Likelihood ratio,Point null,Positive predictive value},
  number = {sup1}
}

@article{colquhoun_reproducibility_2017,
  title = {The Reproducibility of Research and the Misinterpretation of P-Values},
  author = {Colquhoun, David},
  year = {2017},
  volume = {4},
  pages = {171085},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.171085},
  abstract = {We wish to answer this question: If you observe a `significant' p-value after doing a single unbiased experiment, what is the probability that your result is a false positive? The weak evidence provided by p-values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe p\,=\,0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3\,:\,1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the p-value. And if you want to limit the false positive risk to 5\%, you would have to assume that you were 87\% sure that there was a real effect before the experiment was done. If you observe p\,=\,0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100\,:\,1 odds on there being a real effect. That would usually be regarded as conclusive. But the false positive risk would still be 8\% if the prior probability of a real effect were only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5\% you would need to observe p\,=\,0.00045. It is recommended that the terms `significant' and `non-significant' should never be used. Rather, p-values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5\%) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed p-value. Despite decades of warnings, many areas of science still insist on labelling a result of p\,{$<$}\,0.05 as `statistically significant'. This practice must contribute to the lack of reproducibility in some areas of science. This is before you get to the many other well-known problems, like multiple comparisons, lack of randomization and p-hacking. Precise inductive inference is impossible and replication is the only way to be sure. Science is endangered by statistical misunderstanding, and by senior people who impose perverse incentives on scientists.},
  journal = {Royal Society Open Science},
  number = {12}
}

@article{cousins_jeffreyslindley_2017,
  title = {The {{Jeffreys}}\textendash{{Lindley}} Paradox and Discovery Criteria in High Energy Physics},
  author = {Cousins, Robert D.},
  year = {2017},
  month = feb,
  volume = {194},
  pages = {395--432},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-014-0525-z},
  abstract = {The Jeffreys\textendash Lindley paradox displays how the use of a ppp value (or number of standard deviations zzz) in a frequentist hypothesis test can lead to an inference that is radically different from that of a Bayesian hypothesis test in the form advocated by Harold Jeffreys in the 1930s and common today. The setting is the test of a well-specified null hypothesis (such as the Standard Model of elementary particle physics, possibly with ``nuisance parameters'') versus a composite alternative (such as the Standard Model plus a new force of nature of unknown strength). The ppp value, as well as the ratio of the likelihood under the null hypothesis to the maximized likelihood under the alternative, can strongly disfavor the null hypothesis, while the Bayesian posterior probability for the null hypothesis can be arbitrarily large. The academic statistics literature contains many impassioned comments on this paradox, yet there is no consensus either on its relevance to scientific communication or on its correct resolution. The paradox is quite relevant to frontier research in high energy physics. This paper is an attempt to explain the situation to both physicists and statisticians, in the hope that further progress can be made.},
  annotation = {00028},
  journal = {Synthese},
  language = {en},
  number = {2}
}

@article{cowles_origins_1982,
  title = {On the Origins of the. 05 Level of Statistical Significance.},
  author = {Cowles, Michael and Davis, Caroline},
  year = {1982},
  volume = {37},
  pages = {553},
  journal = {American Psychologist},
  number = {5}
}

@article{cumming_replication_2008,
  title = {Replication and {\emph{p}} {{Intervals}}: {\emph{P}} {{Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  shorttitle = {Replication and {\emph{p}} {{Intervals}}},
  author = {Cumming, Geoff},
  year = {2008},
  month = jul,
  volume = {3},
  pages = {286--300},
  issn = {17456916, 17456924},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {4}
}

@article{dienes_four_2017,
  title = {Four Reasons to Prefer {{Bayesian}} Analyses over Significance Testing},
  author = {Dienes, Zoltan and Mclatchie, Neil},
  year = {2017},
  month = mar,
  pages = {1--12},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1266-z},
  abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in mo},
  journal = {Psychonomic Bulletin \& Review},
  language = {en}
}

@article{fiedler_long_2012,
  title = {The {{Long Way From}} {$\alpha$}-{{Error Control}} to {{Validity Proper}}: {{Problems With}} a {{Short}}-{{Sighted False}}-{{Positive Debate}}},
  shorttitle = {The {{Long Way From}} {$\alpha$}-{{Error Control}} to {{Validity Proper}}},
  author = {Fiedler, K. and Kutzner, F. and Krueger, J. I.},
  year = {2012},
  month = nov,
  volume = {7},
  pages = {661--669},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612462587},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {6}
}

@article{field_minimizing_2004,
  title = {Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonz{\'e}n, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  year = {2004},
  volume = {7},
  pages = {669--675},
  journal = {Ecology Letters},
  number = {8}
}

@book{fisher_design_1935,
  title = {The Design of Experiments},
  author = {Fisher, Ronald Aylmer},
  year = {1935},
  publisher = {{Oliver And Boyd; Edinburgh; London}}
}

@article{fisher_introduction_1926,
  title = {Introduction to ``{{The}} Arrangement of Field Experiments''},
  author = {Fisher, R. A.},
  year = {1926},
  volume = {33},
  pages = {503--513},
  journal = {Journal of the Ministry of Agriculture}
}

@article{frick_appropriate_1996,
  title = {The Appropriate Use of Null Hypothesis Testing.},
  author = {Frick, Robert W.},
  year = {1996},
  volume = {1},
  pages = {379},
  journal = {Psychological Methods},
  number = {4}
}

@article{gigerenzer_statistical_2018,
  title = {Statistical {{Rituals}}: {{The Replication Delusion}} and {{How We Got There}}},
  shorttitle = {Statistical {{Rituals}}},
  author = {Gigerenzer, Gerd},
  year = {2018},
  month = jun,
  pages = {2515245918771329},
  issn = {2515-2459},
  doi = {10.1177/2515245918771329},
  abstract = {The ``replication crisis'' has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers' widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The ``null ritual,'' unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 \textendash{} p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20\% of the faculty teaching statistics in psychology, 39\% of the professors and lecturers, and 66\% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 \textendash{} p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56\%\textendash 97\%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as ``significant'' or ``not significant.''},
  annotation = {00000},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en}
}

@article{good_bayes-non-bayes_1992,
  title = {The {{Bayes}}-{{Non}}-{{Bayes Compromise}}: {{A Brief Review}}},
  shorttitle = {The {{Bayes}}/{{Non}}-{{Bayes Compromise}}},
  author = {Good, I. J.},
  year = {1992},
  month = sep,
  volume = {87},
  pages = {597},
  issn = {01621459},
  doi = {10.2307/2290192},
  journal = {Journal of the American Statistical Association},
  number = {419}
}

@article{good_c73._1980,
  title = {C73. {{The}} Diminishing Significance of a p-Value as the Sample Size Increases},
  author = {Good, I. J.},
  year = {1980},
  month = oct,
  volume = {11},
  pages = {307--313},
  issn = {0094-9655},
  doi = {10.1080/00949658008810416},
  journal = {Journal of Statistical Computation and Simulation},
  number = {3-4}
}

@article{good_interface_1988,
  title = {The Interface between Statistics and Philosophy of Science},
  author = {Good, I. J.},
  year = {1988},
  volume = {3},
  pages = {386--397},
  journal = {Statistical Science},
  number = {4}
}

@article{good_lindleys_1982,
  title = {Lindley's Paradox: {{Comment}}},
  shorttitle = {Lindley's Paradox},
  author = {Good, I. J.},
  year = {1982},
  volume = {77},
  pages = {342--344},
  journal = {Journal of the American Statistical Association},
  number = {378}
}

@article{good_standardized_1982,
  title = {Standardized Tail-Area Probabilities},
  author = {Good, I. J.},
  year = {1982},
  month = dec,
  volume = {16},
  pages = {65--66},
  issn = {0094-9655},
  doi = {10.1080/00949658208810607},
  journal = {Journal of Statistical Computation and Simulation},
  number = {1}
}

@article{hamilton_economic_2000,
  title = {An Economic Evaluation of Local Government Approaches to Koala Conservation},
  author = {Hamilton, Clive and Lunney, Daniel and Matthews, Alison},
  year = {2000},
  volume = {7},
  pages = {158--169},
  publisher = {{Taylor \& Francis}},
  journal = {Australian Journal of Environmental Management},
  number = {3}
}

@book{jeffreys_theory_1939,
  title = {Theory of Probability},
  author = {Jeffreys, Harold},
  year = {1939},
  edition = {1st ed},
  publisher = {{Clarendon Press ; Oxford University Press}},
  address = {{Oxford [Oxfordshire] : New York}},
  isbn = {978-0-19-853193-7},
  keywords = {Probabilities},
  lccn = {QA273 .J4 1983},
  series = {The {{International}} Series of Monographs on Physics}
}

@article{kennedy-shaffer_before_2019,
  title = {Before p {$<$} 0.05 to {{Beyond}} p {$<$} 0.05: {{Using History}} to {{Contextualize}} p-{{Values}} and {{Significance Testing}}},
  shorttitle = {Before p {$<$} 0.05 to {{Beyond}} p {$<$} 0.05},
  author = {{Kennedy-Shaffer}, Lee},
  year = {2019},
  month = mar,
  volume = {73},
  pages = {82--90},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537891},
  abstract = {As statisticians and scientists consider a world beyond p {$<$} 0.05, it is important to not lose sight of how we got to this point. Although significance testing and p-values are often presented as prescriptive procedures, they came about through a process of refinement and extension to other disciplines. Ronald A. Fisher and his contemporaries formalized these methods in the early twentieth century and Fisher's 1925 Statistical Methods for Research Workers brought the techniques to experimentalists in a variety of disciplines. Understanding how these methods arose, spread, and were argued over since then illuminates how p {$<$} 0.05 came to be a standard for scientific inference, the advantage it offered at the time, and how it was interpreted. This historical perspective can inform the work of statisticians today by encouraging thoughtful consideration of how their work, including proposed alternatives to the p-value, will be perceived and used by scientists. And it can engage students more fully and encourage critical thinking rather than rote applications of formulae. Incorporating history enables students, practitioners, and statisticians to treat the discipline as an ongoing endeavor, crafted by fallible humans, and provides a deeper understanding of the subject and its consequences for science and society.},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1537891},
  journal = {The American Statistician},
  keywords = {Education,Foundational issues,Hypothesis testing,Inference,Probability},
  number = {sup1},
  pmid = {31413381}
}

@article{lakens_equivalence_2018,
  title = {Equivalence Testing for Psychological Research: {{A}} Tutorial},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  volume = {1},
  pages = {259--269},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@article{lakens_justify_2018,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and Oliveira, Cilene Lino and Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = feb,
  volume = {2},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  annotation = {00010},
  copyright = {2018 The Publisher},
  journal = {Nature Human Behaviour},
  language = {en}
}

@book{leamer_specification_1978,
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  shorttitle = {Specification {{Searches}}},
  author = {Leamer, Edward E.},
  year = {1978},
  month = apr,
  edition = {1 edition},
  publisher = {{Wiley}},
  address = {{New York usw.}},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  isbn = {978-0-471-01520-8},
  language = {English}
}

@article{lindley_statistical_1957,
  title = {A Statistical Paradox},
  author = {Lindley, Dennis V.},
  year = {1957},
  volume = {44},
  pages = {187--192},
  journal = {Biometrika},
  number = {1/2}
}

@article{miller_quest_2019,
  title = {The Quest for an Optimal Alpha},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2019},
  month = jan,
  volume = {14},
  pages = {e0208631},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0208631},
  abstract = {Researchers who analyze data within the framework of null hypothesis significance testing must choose a critical ``alpha'' level, {$\alpha$}, to use as a cutoff for deciding whether a given set of data demonstrates the presence of a particular effect. In most fields, {$\alpha$} = 0.05 has traditionally been used as the standard cutoff. Many researchers have recently argued for a change to a more stringent evidence cutoff such as {$\alpha$} = 0.01, 0.005, or 0.001, noting that this change would tend to reduce the rate of false positives, which are of growing concern in many research areas. Other researchers oppose this proposed change, however, because it would correspondingly tend to increase the rate of false negatives. We show how a simple statistical model can be used to explore the quantitative tradeoff between reducing false positives and increasing false negatives. In particular, the model shows how the optimal {$\alpha$} level depends on numerous characteristics of the research area, and it reveals that although {$\alpha$} = 0.05 would indeed be approximately the optimal value in some realistic situations, the optimal {$\alpha$} could actually be substantially larger or smaller in other situations. The importance of the model lies in making it clear what characteristics of the research area have to be specified to make a principled argument for using one {$\alpha$} level rather than another, and the model thereby provides a blueprint for researchers seeking to justify a particular {$\alpha$} level.},
  annotation = {00000},
  journal = {PLOS ONE},
  keywords = {Decision theory,Economic growth,Health economics,Medicine and health sciences,Psychology,Publication ethics,Statistical methods,Statistical models},
  language = {en},
  number = {1}
}

@article{mudge_setting_2012,
  title = {Setting an {{Optimal}} {$\alpha$} {{That Minimizes Errors}} in {{Null Hypothesis Significance Tests}}},
  author = {Mudge, Joseph F. and Baker, Leanne F. and Edge, Christopher B. and Houlahan, Jeff E.},
  year = {2012},
  month = feb,
  volume = {7},
  pages = {e32734},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0032734},
  abstract = {Null hypothesis significance testing has been under attack in recent years, partly owing to the arbitrary nature of setting {$\alpha$} (the decision-making threshold and probability of Type I error) at a constant value, usually 0.05. If the goal of null hypothesis testing is to present conclusions in which we have the highest possible confidence, then the only logical decision-making threshold is the value that minimizes the probability (or occasionally, cost) of making errors. Setting {$\alpha$} to minimize the combination of Type I and Type II error at a critical effect size can easily be accomplished for traditional statistical tests by calculating the {$\alpha$} associated with the minimum average of {$\alpha$} and {$\beta$} at the critical effect size. This technique also has the flexibility to incorporate prior probabilities of null and alternate hypotheses and/or relative costs of Type I and Type II errors, if known. Using an optimal {$\alpha$} results in stronger scientific inferences because it estimates and minimizes both Type I errors and relevant Type II errors for a test. It also results in greater transparency concerning assumptions about relevant effect size(s) and the relative costs of Type I and II errors. By contrast, the use of {$\alpha$} = 0.05 results in arbitrary decisions about what effect sizes will likely be considered significant, if real, and results in arbitrary amounts of Type II error for meaningful potential effect sizes. We cannot identify a rationale for continuing to arbitrarily use {$\alpha$} = 0.05 for null hypothesis significance tests in any field, when it is possible to determine an optimal {$\alpha$}.},
  journal = {PLOS ONE},
  keywords = {Agricultural soil science,Decision making,Experimental design,Freshwater fish,Gene expression,Lakes,Research errors,Shores},
  number = {2}
}

@article{neyman_problem_1933,
  title = {On the {{Problem}} of the {{Most Efficient Tests}} of {{Statistical Hypotheses}}},
  author = {Neyman, J. and Pearson, E. S.},
  year = {1933},
  month = jan,
  volume = {231},
  pages = {289--337},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.1933.0009},
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {694-706}
}

@article{oberauer_addressing_2019,
  title = {Addressing the Theory Crisis in Psychology},
  author = {Oberauer, Klaus and Lewandowsky, Stephan},
  year = {2019},
  volume = {26},
  pages = {1596--1618},
  publisher = {{Springer}},
  journal = {Psychonomic bulletin \& review},
  number = {5}
}

@book{ravetz_scientific_1995,
  title = {Scientific {{Knowledge}} and {{Its Social Problems}}},
  author = {Ravetz, Jerome},
  year = {1995},
  month = jan,
  edition = {Reprint edition},
  publisher = {{Transaction Publishers}},
  address = {{New Brunswick, N.J}},
  abstract = {Science is continually confronted by new and difficult social and ethical problems. Some of these problems have arisen from the transformation of the academic science of the prewar period into the industrialized science of the present. Traditional theories of science are now widely recognized as obsolete. In Scientific Knowledge and Its Social Problems (originally published in 1971), Jerome R. Ravetz analyzes the work of science as the creation and investigation of problems. He demonstrates the role of choice and value judgment, and the inevitability of error, in scientific research. Ravetz's new introductory essay is a masterful statement of how our understanding of science has evolved over the last two decades.},
  isbn = {978-1-56000-851-4},
  language = {English}
}

@article{rouder_bayesian_2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  volume = {16},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  annotation = {01475},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {2}
}

@article{sellke_calibration_2001,
  title = {Calibration of {$\rho$} Values for Testing Precise Null Hypotheses},
  author = {Sellke, Thomas and Bayarri, M. J. and Berger, James O.},
  year = {2001},
  volume = {55},
  pages = {62--71},
  journal = {The American Statistician},
  number = {1}
}

@article{senn_two_2001,
  title = {Two Cheers for {{P}}-Values?},
  author = {Senn, S.},
  year = {2001},
  volume = {6},
  pages = {193--204},
  journal = {Journal of Epidemiology and Biostatistics},
  number = {2}
}

@article{skipper_sacredness_1967,
  title = {The {{Sacredness}} of .05: {{A Note}} Concerning the {{Uses}} of {{Statistical Levels}} of {{Significance}} in {{Social Science}}},
  shorttitle = {The {{Sacredness}} of .05},
  author = {Skipper, James K. and Guenther, Anthony L. and Nass, Gilbert},
  year = {1967},
  volume = {2},
  pages = {16--18},
  issn = {0003-1232},
  journal = {The American Sociologist},
  number = {1}
}

@article{wagenmakers_bayesian_2016,
  title = {Bayesian Benefits for the Pragmatic Researcher},
  author = {Wagenmakers, Eric-Jan and Morey, Richard D. and Lee, Michael D.},
  year = {2016},
  volume = {25},
  pages = {169--176},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  journal = {Current Directions in Psychological Science},
  number = {3}
}

@book{winer_statistical_1962,
  title = {Statistical Principles in Experimental Design},
  author = {Winer, B. J},
  year = {1962},
  publisher = {{New York : McGraw-Hill}},
  abstract = {"Written primarily for students and research workers in the area of the behavioral sciences, this book is meant to provide a text and comprehensive reference source on statistical principles underlying experimental design. Particular emphasis is given to those designs that are likely to prove useful in research in the behavioral sciences. The book primarily emphasizes the logical basis of principles underlying designs for experiments rather than mathematical derivations associated with relevant sampling distributions. The topics selected for inclusion are those covered in courses taught by the author during the past several years. Students in these courses have widely varying backgrounds in mathematics and come primarily from the fields of psychology, education, economics, sociology, and industrial engineering. It has been the intention of the author to keep the book at a readability level appropriate for students having a mathematical background equivalent to freshman college algebra. From experience with those sections of the book which have been used as text material in dittoed form, there is evidence to indicate that, in large measure, the desired readability level has been attained. Admittedly, however, there are some sections in the book where this readability goal has not been achieved. The first course in design, as taught by the author, has as a prerequisite a basic course in statistical inference. The contents of Chaps. 1 and 2 review the highlights of what is included in the prerequisite material. These chapters are not meant to provide the reader with a first exposure to these topics. They are intended to provide a review of terminology and notation for the concepts which are more fully developed in later chapters. By no means is all the material included in the book covered in a one semester course. In a course of this length, the author has included Chaps. 3, 4, parts of 5, 6, parts of 7, parts of 10, and parts of 11. Chapters 8 through 11 were written to be somewhat independent of each other. Hence one may read, with understanding, in these chapters without undue reference to material in the others. In general, the discussion of principles, interpretations of illustrative examples, and computational procedures are included in successive sections within the same chapter. However, to facilitate the use of the book as a reference source, this procedure is not followed in Chaps. 5 and 6. Basic principles associated with a large class of designs for factorial experiments are discussed in Chap. 5. Detailed illustrative examples of these designs are presented in Chap. 6. For teaching purposes, the author includes relevant material from Chap. 6 with the corresponding material in Chap. 5. Selected topics from Chaps. 7 through 11 have formed the basis for a second course in experimental design. Relatively complete tables for sampling distributions of statistics used in the analysis of experimental designs are included in the Appendix. Ample references to source materials having mathematical proofs for the principles stated in the text are provided"--Preface. (PsycINFO Database Record (c) 2008 APA, all rights reserved).  x, 672 p. : ill. ; 24 cm. Social sciences -- Statistical methods. Psychology -- Statistical methods. Research. Plan d'exp\'erience. Experimentelle Psychologie. Statistik. Versuchsplanung. Psychology -- Research. Social sciences -- Research. Experimenteel ontwerp. Variantieanalyse. Experimental design. Statistics as Topic.},
  language = {English}
}

@book{zellner_introduction_1971,
  title = {An Introduction to {{Bayesian}} Inference in Econometrics},
  author = {Zellner, Arnold},
  year = {1971},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-98165-7},
  keywords = {Bayesian statistical decision theory,Econometrics},
  lccn = {HB74.M3 Z44},
  series = {Wiley Series in Probability and Mathematical Statistics}
}

@article{berger_testing_1987,
  title = {Testing a Point Null Hypothesis: The Irreconcilability of {{P}} Values and Evidence},
  volume = {82},
  shorttitle = {Testing a Point Null Hypothesis},
  number = {397},
  journal = {Journal of the American statistical Association},
  author = {Berger, James O. and Sellke, Thomas},
  year = {1987},
  pages = {112--122}
}

@article{gigerenzer_statistical_2018,
  title = {Statistical {{Rituals}}: {{The Replication Delusion}} and {{How We Got There}}},
  issn = {2515-2459},
  shorttitle = {Statistical {{Rituals}}},
  abstract = {The ``replication crisis'' has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers' widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The ``null ritual,'' unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 \textendash{} p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20\% of the faculty teaching statistics in psychology, 39\% of the professors and lecturers, and 66\% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 \textendash{} p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56\%\textendash{}97\%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as ``significant'' or ``not significant.''},
  language = {en},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245918771329},
  author = {Gigerenzer, Gerd},
  month = jun,
  year = {2018},
  pages = {2515245918771329},
  note = {00000}
}

@article{bakan_test_1966,
  title = {The Test of Significance in Psychological Research.},
  volume = {66},
  number = {6},
  journal = {Psychological bulletin},
  author = {Bakan, David},
  year = {1966},
  pages = {423-437}
}

@article{skipper_sacredness_1967,
  title = {The {{Sacredness}} of .05: {{A Note}} Concerning the {{Uses}} of {{Statistical Levels}} of {{Significance}} in {{Social Science}}},
  volume = {2},
  issn = {0003-1232},
  shorttitle = {The {{Sacredness}} of .05},
  number = {1},
  journal = {The American Sociologist},
  author = {Skipper, James K. and Guenther, Anthony L. and Nass, Gilbert},
  year = {1967},
  pages = {16-18}
}

@article{cowles_origins_1982,
  title = {On the Origins of the. 05 Level of Statistical Significance.},
  volume = {37},
  number = {5},
  journal = {American Psychologist},
  author = {Cowles, Michael and Davis, Caroline},
  year = {1982},
  pages = {553}
}

@book{fisher_design_1935,
  title = {The Design of Experiments},
  publisher = {{Oliver And Boyd; Edinburgh; London}},
  author = {Fisher, Ronald Aylmer},
  year = {1935}
}

@article{fisher_introduction_1926,
  title = {Introduction to ``{{The}} Arrangement of Field Experiments''},
  volume = {33},
  journal = {Journal of the Ministry of Agriculture},
  author = {Fisher, R. A.},
  year = {1926},
  pages = {503--513}
}

@article{lindley_statistical_1957,
  title = {A Statistical Paradox},
  volume = {44},
  number = {1/2},
  journal = {Biometrika},
  author = {Lindley, Dennis V.},
  year = {1957},
  pages = {187--192}
}

@article{cousins_jeffreyslindley_2017,
  title = {The {{Jeffreys}}\textendash{{Lindley}} Paradox and Discovery Criteria in High Energy Physics},
  volume = {194},
  issn = {0039-7857, 1573-0964},
  abstract = {The Jeffreys\textendash{}Lindley paradox displays how the use of a ppp value (or number of standard deviations zzz) in a frequentist hypothesis test can lead to an inference that is radically different from that of a Bayesian hypothesis test in the form advocated by Harold Jeffreys in the 1930s and common today. The setting is the test of a well-specified null hypothesis (such as the Standard Model of elementary particle physics, possibly with ``nuisance parameters'') versus a composite alternative (such as the Standard Model plus a new force of nature of unknown strength). The ppp value, as well as the ratio of the likelihood under the null hypothesis to the maximized likelihood under the alternative, can strongly disfavor the null hypothesis, while the Bayesian posterior probability for the null hypothesis can be arbitrarily large. The academic statistics literature contains many impassioned comments on this paradox, yet there is no consensus either on its relevance to scientific communication or on its correct resolution. The paradox is quite relevant to frontier research in high energy physics. This paper is an attempt to explain the situation to both physicists and statisticians, in the hope that further progress can be made.},
  language = {en},
  number = {2},
  journal = {Synthese},
  doi = {10.1007/s11229-014-0525-z},
  author = {Cousins, Robert D.},
  month = feb,
  year = {2017},
  pages = {395-432},
  note = {00028}
}

@article{neyman_problem_1933,
  title = {On the {{Problem}} of the {{Most Efficient Tests}} of {{Statistical Hypotheses}}},
  volume = {231},
  issn = {1364-503X, 1471-2962},
  language = {en},
  number = {694-706},
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  doi = {10.1098/rsta.1933.0009},
  author = {Neyman, J. and Pearson, E. S.},
  month = jan,
  year = {1933},
  pages = {289-337}
}

@book{cohen_statistical_1988,
  address = {{Hillsdale, N.J}},
  edition = {2nd ed},
  title = {Statistical Power Analysis for the Behavioral Sciences},
  isbn = {978-0-8058-0283-2},
  lccn = {HA29 .C66 1988},
  publisher = {{L. Erlbaum Associates}},
  author = {Cohen, Jacob},
  year = {1988},
  keywords = {Statistical methods,Social sciences,Probabilities,Statistical power analysis}
}

@book{leamer_specification_1978,
  address = {{New York usw.}},
  edition = {1 edition},
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  isbn = {978-0-471-01520-8},
  shorttitle = {Specification {{Searches}}},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  language = {English},
  publisher = {{Wiley}},
  author = {Leamer, Edward E.},
  month = apr,
  year = {1978}
}

@article{rouder_bayesian_2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  volume = {16},
  issn = {1069-9384, 1531-5320},
  language = {en},
  number = {2},
  journal = {Psychonomic Bulletin \& Review},
  doi = {10.3758/PBR.16.2.225},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  month = apr,
  year = {2009},
  pages = {225-237},
  note = {01475}
}

@book{jeffreys_theory_1939,
  address = {{Oxford [Oxfordshire] : New York}},
  edition = {1st ed},
  series = {The {{International}} Series of Monographs on Physics},
  title = {Theory of Probability},
  isbn = {978-0-19-853193-7},
  lccn = {QA273 .J4 1983},
  publisher = {{Clarendon Press ; Oxford University Press}},
  author = {Jeffreys, Harold},
  year = {1939},
  keywords = {Probabilities}
}

@article{berger_unified_1997,
  title = {Unified Frequentist and {{Bayesian}} Testing of a Precise Hypothesis},
  volume = {12},
  number = {3},
  journal = {Statistical Science},
  author = {Berger, James O. and Boukai, Ben and Wang, Yinping and others},
  year = {1997},
  pages = {133--160}
}

@article{benjamin_redefine_2018,
  title = {Redefine Statistical Significance},
  volume = {2},
  copyright = {2017 The Author(s)},
  issn = {2397-3374},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  language = {en},
  number = {1},
  journal = {Nature Human Behaviour},
  doi = {10.1038/S41562-017-0189-Z},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and Boeck, Paul De and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Ho, Teck Hua and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Zandt, Trisha Van and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  month = jan,
  year = {2018},
  pages = {6-10},
  note = {00170}
}

@article{sellke_calibration_2001,
  title = {Calibration of {$\rho$} Values for Testing Precise Null Hypotheses},
  volume = {55},
  number = {1},
  journal = {The American Statistician},
  author = {Sellke, Thomas and Bayarri, M. J. and Berger, James O.},
  year = {2001},
  pages = {62--71}
}

@article{senn_two_2001,
  title = {Two Cheers for {{P}}-Values?},
  volume = {6},
  number = {2},
  journal = {Journal of Epidemiology and Biostatistics},
  author = {Senn, S.},
  year = {2001},
  pages = {193--204}
}

@article{mudge_setting_2012,
  title = {Setting an {{Optimal}} {$\alpha$} {{That Minimizes Errors}} in {{Null Hypothesis Significance Tests}}},
  volume = {7},
  issn = {1932-6203},
  abstract = {Null hypothesis significance testing has been under attack in recent years, partly owing to the arbitrary nature of setting {$\alpha$} (the decision-making threshold and probability of Type I error) at a constant value, usually 0.05. If the goal of null hypothesis testing is to present conclusions in which we have the highest possible confidence, then the only logical decision-making threshold is the value that minimizes the probability (or occasionally, cost) of making errors. Setting {$\alpha$} to minimize the combination of Type I and Type II error at a critical effect size can easily be accomplished for traditional statistical tests by calculating the {$\alpha$} associated with the minimum average of {$\alpha$} and {$\beta$} at the critical effect size. This technique also has the flexibility to incorporate prior probabilities of null and alternate hypotheses and/or relative costs of Type I and Type II errors, if known. Using an optimal {$\alpha$} results in stronger scientific inferences because it estimates and minimizes both Type I errors and relevant Type II errors for a test. It also results in greater transparency concerning assumptions about relevant effect size(s) and the relative costs of Type I and II errors. By contrast, the use of {$\alpha$} = 0.05 results in arbitrary decisions about what effect sizes will likely be considered significant, if real, and results in arbitrary amounts of Type II error for meaningful potential effect sizes. We cannot identify a rationale for continuing to arbitrarily use {$\alpha$} = 0.05 for null hypothesis significance tests in any field, when it is possible to determine an optimal {$\alpha$}.},
  number = {2},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0032734},
  author = {Mudge, Joseph F. and Baker, Leanne F. and Edge, Christopher B. and Houlahan, Jeff E.},
  month = feb,
  year = {2012},
  keywords = {Decision making,Experimental design,Freshwater fish,Lakes,Research errors,Gene expression,Shores,Agricultural soil science},
  pages = {e32734}
}

@book{winer_statistical_1962,
  title = {Statistical Principles in Experimental Design},
  abstract = {"Written primarily for students and research workers in the area of the behavioral sciences, this book is meant to provide a text and comprehensive reference source on statistical principles underlying experimental design. Particular emphasis is given to those designs that are likely to prove useful in research in the behavioral sciences. The book primarily emphasizes the logical basis of principles underlying designs for experiments rather than mathematical derivations associated with relevant sampling distributions. The topics selected for inclusion are those covered in courses taught by the author during the past several years. Students in these courses have widely varying backgrounds in mathematics and come primarily from the fields of psychology, education, economics, sociology, and industrial engineering. It has been the intention of the author to keep the book at a readability level appropriate for students having a mathematical background equivalent to freshman college algebra. From experience with those sections of the book which have been used as text material in dittoed form, there is evidence to indicate that, in large measure, the desired readability level has been attained. Admittedly, however, there are some sections in the book where this readability goal has not been achieved. The first course in design, as taught by the author, has as a prerequisite a basic course in statistical inference. The contents of Chaps. 1 and 2 review the highlights of what is included in the prerequisite material. These chapters are not meant to provide the reader with a first exposure to these topics. They are intended to provide a review of terminology and notation for the concepts which are more fully developed in later chapters. By no means is all the material included in the book covered in a one semester course. In a course of this length, the author has included Chaps. 3, 4, parts of 5, 6, parts of 7, parts of 10, and parts of 11. Chapters 8 through 11 were written to be somewhat independent of each other. Hence one may read, with understanding, in these chapters without undue reference to material in the others. In general, the discussion of principles, interpretations of illustrative examples, and computational procedures are included in successive sections within the same chapter. However, to facilitate the use of the book as a reference source, this procedure is not followed in Chaps. 5 and 6. Basic principles associated with a large class of designs for factorial experiments are discussed in Chap. 5. Detailed illustrative examples of these designs are presented in Chap. 6. For teaching purposes, the author includes relevant material from Chap. 6 with the corresponding material in Chap. 5. Selected topics from Chaps. 7 through 11 have formed the basis for a second course in experimental design. Relatively complete tables for sampling distributions of statistics used in the analysis of experimental designs are included in the Appendix. Ample references to source materials having mathematical proofs for the principles stated in the text are provided"--Preface. (PsycINFO Database Record (c) 2008 APA, all rights reserved).  x, 672 p. : ill. ; 24 cm. Social sciences -- Statistical methods. Psychology -- Statistical methods. Research. Plan d'exp{\'e}rience. Experimentelle Psychologie. Statistik. Versuchsplanung. Psychology -- Research. Social sciences -- Research. Experimenteel ontwerp. Variantieanalyse. Experimental design. Statistics as Topic.},
  language = {English},
  publisher = {{New York : McGraw-Hill}},
  author = {Winer, B. J},
  year = {1962}
}

@article{miller_quest_2019,
  title = {The Quest for an Optimal Alpha},
  volume = {14},
  issn = {1932-6203},
  abstract = {Researchers who analyze data within the framework of null hypothesis significance testing must choose a critical ``alpha'' level, {$\alpha$}, to use as a cutoff for deciding whether a given set of data demonstrates the presence of a particular effect. In most fields, {$\alpha$} = 0.05 has traditionally been used as the standard cutoff. Many researchers have recently argued for a change to a more stringent evidence cutoff such as {$\alpha$} = 0.01, 0.005, or 0.001, noting that this change would tend to reduce the rate of false positives, which are of growing concern in many research areas. Other researchers oppose this proposed change, however, because it would correspondingly tend to increase the rate of false negatives. We show how a simple statistical model can be used to explore the quantitative tradeoff between reducing false positives and increasing false negatives. In particular, the model shows how the optimal {$\alpha$} level depends on numerous characteristics of the research area, and it reveals that although {$\alpha$} = 0.05 would indeed be approximately the optimal value in some realistic situations, the optimal {$\alpha$} could actually be substantially larger or smaller in other situations. The importance of the model lies in making it clear what characteristics of the research area have to be specified to make a principled argument for using one {$\alpha$} level rather than another, and the model thereby provides a blueprint for researchers seeking to justify a particular {$\alpha$} level.},
  language = {en},
  number = {1},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0208631},
  author = {Miller, Jeff and Ulrich, Rolf},
  month = jan,
  year = {2019},
  keywords = {Statistical methods,Psychology,Medicine and health sciences,Health economics,Publication ethics,Decision theory,Economic growth,Statistical models},
  pages = {e0208631},
  note = {00000}
}

@article{cumming_replication_2008,
  title = {Replication and {\emph{p}} {{Intervals}}: {\emph{P}} {{Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  volume = {3},
  issn = {17456916, 17456924},
  shorttitle = {Replication and {\emph{p}} {{Intervals}}},
  language = {en},
  number = {4},
  journal = {Perspectives on Psychological Science},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  author = {Cumming, Geoff},
  month = jul,
  year = {2008},
  pages = {286-300}
}

@article{dienes_four_2017,
  title = {Four Reasons to Prefer {{Bayesian}} Analyses over Significance Testing},
  issn = {1069-9384, 1531-5320},
  abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in mo},
  language = {en},
  journal = {Psychonomic Bulletin \& Review},
  doi = {10.3758/s13423-017-1266-z},
  author = {Dienes, Zoltan and Mclatchie, Neil},
  month = mar,
  year = {2017},
  pages = {1-12}
}

@article{albers_when_2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  volume = {74},
  issn = {0022-1031},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies.
Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  journal = {Journal of Experimental Social Psychology},
  doi = {10.1016/j.jesp.2017.09.004},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis},
  pages = {187-195}
}

@article{colquhoun2017reproducibility,
  title={The reproducibility of research and the misinterpretation of p-values},
  author={Colquhoun},
  journal={Royal society open science},
  volume={4},
  number={12},
  pages={171085},
  year={2017},
  publisher={The Royal Society Publishing}
}

@article{bem2011feeling,
  title={Feeling the future: experimental evidence for anomalous retroactive influences on cognition and affect.},
  author={Bem, Daryl J},
  journal={Journal of personality and social psychology},
  volume={100},
  number={3},
  pages={407},
  year={2011},
  publisher={American Psychological Association}
}

@article{fiedler2012long,
  title={The long way from $\alpha$-error control to validity proper: Problems with a short-sighted false-positive debate},
  author={Fiedler, Klaus and Kutzner, Florian and Krueger, Joachim I},
  journal={Perspectives on Psychological Science},
  volume={7},
  number={6},
  pages={661--669},
  year={2012},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{colquhoun2019false,
  title={The false positive risk: a proposal concerning what to do about p-values},
  author={Colquhoun, David},
  journal={The American Statistician},
  volume={73},
  number={sup1},
  pages={192--201},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{oberauer2019addressing,
  title={Addressing the theory crisis in psychology},
  author={Oberauer, Klaus and Lewandowsky, Stephan},
  journal={Psychonomic bulletin \& review},
  volume={26},
  number={5},
  pages={1596--1618},
  year={2019},
  publisher={Springer}
}

@article{hamilton2000economic,
  title={An economic evaluation of local government approaches to koala conservation},
  author={Hamilton, Clive and Lunney, Daniel and Matthews, Alison},
  journal={Australian Journal of Environmental Management},
  volume={7},
  number={3},
  pages={158--169},
  year={2000},
  publisher={Taylor \& Francis}
}

@article{lakens2018equivalence,
  title={Equivalence testing for psychological research: A tutorial},
  author={Lakens, Dani{\"e}l and Scheel, Anne M and Isager, Peder M},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={1},
  number={2},
  pages={259--269},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{wagenmakers2016bayesian,
  title={Bayesian benefits for the pragmatic researcher},
  author={Wagenmakers, Eric-Jan and Morey, Richard D and Lee, Michael D},
  journal={Current Directions in Psychological Science},
  volume={25},
  number={3},
  pages={169--176},
  year={2016},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{field2004minimizing,
  title={Minimizing the cost of environmental management decisions by optimizing statistical thresholds},
  author={Field, Scott and Tyre, Andrew J and Jonz{\'e}n, Niclas and Rhodes, Jonathan R and Possingham, Hugh P},
  journal={Ecology Letters},
  volume={7},
  number={8},
  pages={669--675},
  year={2004},
  publisher={Wiley Online Library}
}

@article{faulkenberry2019estimating,
  title={Estimating evidential value from analysis of variance summaries: A comment on Ly et al.(2018)},
  author={Faulkenberry, Thomas J},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={2},
  number={4},
  pages={406--409},
  year={2019},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{rouder2009bayesian,
  title={Bayesian t tests for accepting and rejecting the null hypothesis},
  author={Rouder, Jeffrey N and Speckman, Paul L and Sun, Dongchu and Morey, Richard D and Iverson, Geoffrey},
  journal={Psychonomic bulletin \& review},
  volume={16},
  number={2},
  pages={225--237},
  year={2009},
  publisher={Springer}
}

@BOOK{LeeWagenmakersBayesBook,
  AUTHOR =       {Lee, M. D. and Wagenmakers, Eric-Jan},
  TITLE =        {Bayesian Cognitive Modeling: {A} Practical Course},
  PUBLISHER =    {Cambridge University Press},
  YEAR =         {2013}
}

@BOOK{Jeffreys1939,
  AUTHOR =       {Jeffreys, H.},
  TITLE =        {Theory of Probability},
  PUBLISHER =    {Oxford University Press},
  YEAR =         {1939},
  address =      {Oxford, UK},
  edition =      {1}
}

@book{mcelreath2020statistical,
  title={Statistical rethinking: A Bayesian course with examples in R and Stan},
  author={McElreath, Richard},
  year={2020},
  publisher={CRC press}
}

@article{van2015gender,
  title={Gender contributes to personal research funding success in The Netherlands},
  author={Van der Lee, Romy and Ellemers, Naomi},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={40},
  pages={12349--12353},
  year={2015},
  publisher={National Acad Sciences}
}

@article{bem2011feeling,
  title={Feeling the future: experimental evidence for anomalous retroactive influences on cognition and affect.},
  author={Bem, Daryl J},
  journal={Journal of personality and social psychology},
  volume={100},
  number={3},
  pages={407},
  year={2011},
  publisher={American Psychological Association}
}

@article{ritchie2012failing,
  title={Failing the future: {T}hree unsuccessful attempts to replicate {B}em's {R}etroactive Facilitation of RecallEffect},
  author={Ritchie, Stuart J and Wiseman, Richard and French, Christopher C},
  journal={PloS one},
  volume={7},
  number={3},
  pages={e33423},
  year={2012},
  publisher={Public Library of Science},
  url = {https://doi.org/10.1371/journal.pone.0033423}
}

@article{wagenmakers2011psychologists,
  title={Why psychologists must change the way they analyze their data: the case of {P}si: {C}omment on {B}em (2011).},
  author={Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and Van Der Maas, Han LJ},
  year={2011},
  publisher={American Psychological Association},
  url = {http://dx.doi.org/10.2139/ssrn.2001721}
}

@article{galak2012correcting,
  title={Correcting the past: Failures to replicate {p}si.},
  author={Galak, Jeff and LeBoeuf, Robyn A and Nelson, Leif D and Simmons, Joseph P},
  journal={Journal of personality and social psychology},
  volume={103},
  number={6},
  pages={933},
  year={2012},
  publisher={American Psychological Association},
  url = {https://doi.org/10.1037/a0029709}
}


@techreport{tunc_epistemic_2021,
	title = {The {Epistemic} and {Pragmatic} {Function} of {Dichotomous} {Claims} {Based} on {Statistical} {Hypothesis} {Tests}},
	url = {https://psyarxiv.com/af9by/},
	abstract = {Researchers commonly make dichotomous claims based on continuous test statistics. Many have branded the practice as misuse of statistics, and criticize scientists for suffering from dichotomania. However, the role dichotomous claims play in science is not primarily a statistical one, but an epistemological and pragmatic one. The epistemological function of dichotomous claims consists in transforming data into factual statements that can falsify a universal statement. This transformation requires pre-specified methodological decision procedures such as statistical hypothesis testing (e.g., Neyman-Pearson tests). From the perspective of methodological falsificationism these decision procedures are necessary, as probabilistic statements (e.g. continuous test statistics) cannot function as falsifiers of substantive hypotheses. However, they are not sufficient since for dichotomous claims to have any implication regarding theoretical claims about phenomena, there should be a valid derivation chain linking theoretical, experimental and data models. The pragmatic function of dichotomous claims is facilitating scrutiny and criticism among peers by generating contestable statements, a process referred to by Popper as 'conjectures and refutations', through which we can determine which theories withstand scrutiny the best. Abandoning dichotomous claims to combat the misuse of statistics would not improve scientific inferences but will sacrifice these crucial epistemic and pragmatic functions.},
	urldate = {2021-02-22},
	institution = {PsyArXiv},
	author = {Tun, Duygu Uygun and Tun, Mehmet Necip and Lakens, Daniel},
	month = feb,
	year = {2021},
	doi = {10.31234/osf.io/af9by},
	note = {type: article},
	keywords = {Quantitative Methods, Social and Behavioral Sciences, Theory and Philosophy of Science, basic statements, dichotomous claims, methodological falsificationism, statisitical hypothesis testing, theory testing},
	file = {Tun et al_2021_The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\NUIJP6U7\\Tun et al_2021_The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical.pdf:application/pdf}
}


@article{douglas_inductive_2000,
	title = {Inductive risk and values in science},
	volume = {67},
	number = {4},
	journal = {Philosophy of science},
	author = {Douglas, Heather E.},
	year = {2000},
	note = {Publisher: University of Chicago Press},
	pages = {559--579},
	file = {Douglas_2000_Inductive risk and values in science.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\5LNEDQS2\\Douglas_2000_Inductive risk and values in science.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\MI8QQN3U\\392855.html:text/html}
}


@techreport{lakens_sample_2021,
	title = {Sample {Size} {Justification}},
	url = {https://psyarxiv.com/9d3yf/},
	abstract = {An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
	urldate = {2021-01-04},
	institution = {PsyArXiv},
	author = {Lakens, Danil},
	month = jan,
	year = {2021},
	doi = {10.31234/osf.io/9d3yf},
	note = {type: article},
	keywords = {power analysis, value of information, Experimental Design and Sample Surveys, Quantitative Methods, Social and Behavioral Sciences, sample size justification, study design},
	file = {Lakens_2021_Sample Size Justification.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\ENBKIT5E\\Lakens_2021_Sample Size Justification.pdf:application/pdf}
}


@article{erdfelder_gpower_1996,
	title = {{GPOWER}: {A} general power analysis program},
	volume = {28},
	issn = {0743-3808, 1532-5970},
	shorttitle = {{GPOWER}},
	url = {http://link.springer.com/10.3758/BF03203630},
	doi = {10.3758/BF03203630},
	language = {en},
	number = {1},
	urldate = {2020-08-04},
	journal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
	month = mar,
	year = {1996},
	pages = {1--11},
	file = {Erdfelder et al. - 1996 - GPOWER A general power analysis program.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\8MASB4SB\\Erdfelder et al. - 1996 - GPOWER A general power analysis program.pdf:application/pdf}
}


@book{fisher_design_1971,
	address = {New York},
	edition = {9 edition},
	title = {The {Design} of {Experiments}},
	isbn = {978-0-02-844690-5},
	language = {English},
	publisher = {Macmillan Pub Co},
	author = {Fisher, Ronald A.},
	month = jun,
	year = {1971},
	file = {Fisher - 1971 - The Design of Experiments.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\6X5NI5NV\\Fisher - 1971 - The Design of Experiments.pdf:application/pdf}
}

@article{kim2021choosing,
  title={Choosing the level of significance: A decision-theoretic approach},
  author={Kim, Jae H and Choi, In},
  journal={Abacus},
  volume={57},
  number={1},
  pages={27--71},
  year={2021},
  publisher={Wiley Online Library}, 
  doi = {10.1111/abac.12172}
}

@article{fiedler2012long,
  title={The long way from $\alpha$-error control to validity proper: Problems with a short-sighted false-positive debate},
  author={Fiedler, Klaus and Kutzner, Florian and Krueger, Joachim I},
  journal={Perspectives on Psychological Science},
  volume={7},
  number={6},
  pages={661--669},
  year={2012},
  url= {10.1177/1745691612462587}
}

@article{albers_when_2018,
  title = {When power analyses based on pilot data are biased: {{Inaccurate}} effect size estimators and follow-up bias},
  author = {Albers, Casper and Lakens, Daniëcoll},
  year = {2018},
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis}
}

@article{bakan_test_1966,
  title = {The test of significance in psychological research},
  author = {Bakan, David},
  year = {1966},
  volume = {66},
  pages = {423--437},
  journal = {Psychological Bulletin},
  number = {6}, 
  doi = {10.1037/h0020412}
}

@article{colquhoun_false_2019,
  title = {The false positive risk: A proposal concerning what to do about p-values},
  author = {Colquhoun, David},
  year = {2019},
  month = mar,
  volume = {73},
  pages = {192--201},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1529622},
  abstract = {It is widely acknowledged that the biomedical literature suffers from a surfeit of false positive results. Part of the reason for this is the persistence of the myth that observation of p {$<$} 0.05 is sufficient justification to claim that you have made a discovery. It is hopeless to expect users to change their reliance on p-values unless they are offered an alternative way of judging the reliability of their conclusions. If the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. One such proposal is based on calculation of false positive risk(FPR). It is suggested that p-values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p-value. This number could be the minimum FPR (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). Alternatively one could specify the prior probability that it would be necessary to believe in order to achieve an FPR of, say, 0.05.},
  journal = {The American Statistician},
  keywords = {Bayes,False positive,False positive report probability,False positive risk,FPR,Likelihood ratio,Point null,Positive predictive value},
  number = {sup1}
}

@article{colquhoun_reproducibility_2017,
  title = {The Reproducibility of Research and the Misinterpretation of P-Values},
  author = {Colquhoun, David},
  year = {2017},
  volume = {4},
  pages = {171085},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.171085},
  abstract = {We wish to answer this question: If you observe a `significant' p-value after doing a single unbiased experiment, what is the probability that your result is a false positive? The weak evidence provided by p-values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe p\,=\,0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3\,:\,1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the p-value. And if you want to limit the false positive risk to 5\%, you would have to assume that you were 87\% sure that there was a real effect before the experiment was done. If you observe p\,=\,0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100\,:\,1 odds on there being a real effect. That would usually be regarded as conclusive. But the false positive risk would still be 8\% if the prior probability of a real effect were only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5\% you would need to observe p\,=\,0.00045. It is recommended that the terms `significant' and `non-significant' should never be used. Rather, p-values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5\%) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed p-value. Despite decades of warnings, many areas of science still insist on labelling a result of p\,{$<$}\,0.05 as `statistically significant'. This practice must contribute to the lack of reproducibility in some areas of science. This is before you get to the many other well-known problems, like multiple comparisons, lack of randomization and p-hacking. Precise inductive inference is impossible and replication is the only way to be sure. Science is endangered by statistical misunderstanding, and by senior people who impose perverse incentives on scientists.},
  journal = {Royal Society Open Science},
  number = {12}
}

@article{cousins_jeffreyslindley_2017,
  title = {The {{Jeffreys}}\textendash{{Lindley}} paradox and discovery criteria in high Energy physics},
  author = {Cousins, Robert D.},
  year = {2017},
  month = feb,
  volume = {194},
  pages = {395--432},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-014-0525-z},
  abstract = {The Jeffreys\textendash Lindley paradox displays how the use of a ppp value (or number of standard deviations zzz) in a frequentist hypothesis test can lead to an inference that is radically different from that of a Bayesian hypothesis test in the form advocated by Harold Jeffreys in the 1930s and common today. The setting is the test of a well-specified null hypothesis (such as the Standard Model of elementary particle physics, possibly with ``nuisance parameters'') versus a composite alternative (such as the Standard Model plus a new force of nature of unknown strength). The ppp value, as well as the ratio of the likelihood under the null hypothesis to the maximized likelihood under the alternative, can strongly disfavor the null hypothesis, while the Bayesian posterior probability for the null hypothesis can be arbitrarily large. The academic statistics literature contains many impassioned comments on this paradox, yet there is no consensus either on its relevance to scientific communication or on its correct resolution. The paradox is quite relevant to frontier research in high energy physics. This paper is an attempt to explain the situation to both physicists and statisticians, in the hope that further progress can be made.},
  annotation = {00028},
  journal = {Synthese},
  language = {en},
  number = {2}
}



@article{dienes_four_2017,
  title = {Four reasons to prefer {{Bayesian}} analyses over significance testing},
  author = {Dienes, Zoltan and Mclatchie, Neil},
  year = {2017},
  month = mar,
  pages = {1--12},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1266-z},
  abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in mo},
  journal = {Psychonomic Bulletin \& Review},
  language = {en}
}

@article{fiedler_long_2012,
  title = {The long way from {$\alpha$}-error control to validity proper: {Problems with} a short-sighted false-positive debate},
  author = {Fiedler, K. and Kutzner, F. and Krueger, J. I.},
  year = {2012},
  month = nov,
  volume = {7},
  pages = {661--669},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612462587},
  journal = {Perspectives on Psychological Science},
  number = {661--669}
}

@article{field_minimizing_2004,
  title = {Minimizing the cost of environmental management decisions by optimizing statistical thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonz{\'e}n, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  year = {2004},
  volume = {7},
  pages = {669--675},
  journal = {Ecology Letters},
  number = {8}, 
  url = {10.1111/j.1461-0248.2004.00625.x}
}

@book{fisher_design_1935,
  title = {The Design of Experiments},
  author = {Fisher, Ronald Aylmer},
  year = {1935},
  publisher = {{Oliver And Boyd; Edinburgh; London}}
}

@article{fisher_introduction_1926,
  title = {Introduction to ``{{The}} arrangement of field experiments''},
  author = {Fisher, R. A.},
  year = {1926},
  volume = {33},
  pages = {503--513},
  journal = {Journal of the Ministry of Agriculture}
}

@article{frick_appropriate_1996,
  title = {The appropriate use of null hypothesis testing},
  author = {Frick, Robert W.},
  year = {1996},
  volume = {1},
  pages = {379--390},
  journal = {Psychological Methods},
  doi = {10.1037/1082-989X.1.4.379},
  number = {4}
}

@article{gigerenzer_statistical_2018,
  title = {Statistical {{Rituals}}: {{The Replication Delusion}} and {{How We Got There}}},
  shorttitle = {Statistical {{Rituals}}},
  author = {Gigerenzer, Gerd},
  year = {2018},
  month = jun,
  pages = {2515245918771329},
  issn = {2515-2459},
  doi = {10.1177/2515245918771329},
  abstract = {The ``replication crisis'' has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers' widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The ``null ritual,'' unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 \textendash{} p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20\% of the faculty teaching statistics in psychology, 39\% of the professors and lecturers, and 66\% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 \textendash{} p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56\%\textendash 97\%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as ``significant'' or ``not significant.''},
  annotation = {00000},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en}
}

@article{good_bayes-non-bayes_1992,
  title = {The {{Bayes}}-{{Non}}-{{Bayes Compromise}}: {{A Brief Review}}},
  shorttitle = {The {{Bayes}}/{{Non}}-{{Bayes Compromise}}},
  author = {Good, I. J.},
  year = {1992},
  month = sep,
  volume = {87},
  pages = {597},
  issn = {01621459},
  doi = {10.2307/2290192},
  journal = {Journal of the American Statistical Association},
  number = {419}
}

@article{good_c73._1980,
  title = {C73. {{The}} Diminishing Significance of a p-Value as the Sample Size Increases},
  author = {Good, I. J.},
  year = {1980},
  month = oct,
  volume = {11},
  pages = {307--313},
  issn = {0094-9655},
  doi = {10.1080/00949658008810416},
  journal = {Journal of Statistical Computation and Simulation},
  number = {3-4}
}

@article{good_interface_1988,
  title = {The Interface between Statistics and Philosophy of Science},
  author = {Good, I. J.},
  year = {1988},
  volume = {3},
  pages = {386--397},
  journal = {Statistical Science},
  number = {4}
}

@article{good_lindleys_1982,
  title = {Lindley's Paradox: {{Comment}}},
  shorttitle = {Lindley's Paradox},
  author = {Good, I. J.},
  year = {1982},
  volume = {77},
  pages = {342--344},
  journal = {Journal of the American Statistical Association},
  number = {378}
}

@article{good_standardized_1982,
  title = {Standardized Tail-Area Probabilities},
  author = {Good, I. J.},
  year = {1982},
  month = dec,
  volume = {16},
  pages = {65--66},
  issn = {0094-9655},
  doi = {10.1080/00949658208810607},
  journal = {Journal of Statistical Computation and Simulation},
  number = {1}
}

@article{hamilton_economic_2000,
  title = {An Economic Evaluation of Local Government Approaches to Koala Conservation},
  author = {Hamilton, Clive and Lunney, Daniel and Matthews, Alison},
  year = {2000},
  volume = {7},
  pages = {158--169},
  publisher = {{Taylor \& Francis}},
  journal = {Australian Journal of Environmental Management},
  number = {3}
}


@article{kennedy-shaffer_before_2019,
  title = {Before p {$<$} 0.05 to beyond p {$<$} 0.05: using history to contextualize p-values and significance testing},
  shorttitle = {Before p {$<$} 0.05 to {{Beyond}} p {$<$} 0.05},
  author = {{Kennedy-Shaffer}, Lee},
  year = {2019},
  month = mar,
  volume = {73},
  pages = {82--90},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537891},
  abstract = {As statisticians and scientists consider a world beyond p {$<$} 0.05, it is important to not lose sight of how we got to this point. Although significance testing and p-values are often presented as prescriptive procedures, they came about through a process of refinement and extension to other disciplines. Ronald A. Fisher and his contemporaries formalized these methods in the early twentieth century and Fisher's 1925 Statistical Methods for Research Workers brought the techniques to experimentalists in a variety of disciplines. Understanding how these methods arose, spread, and were argued over since then illuminates how p {$<$} 0.05 came to be a standard for scientific inference, the advantage it offered at the time, and how it was interpreted. This historical perspective can inform the work of statisticians today by encouraging thoughtful consideration of how their work, including proposed alternatives to the p-value, will be perceived and used by scientists. And it can engage students more fully and encourage critical thinking rather than rote applications of formulae. Incorporating history enables students, practitioners, and statisticians to treat the discipline as an ongoing endeavor, crafted by fallible humans, and provides a deeper understanding of the subject and its consequences for science and society.},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1537891},
  journal = {The American Statistician},
  keywords = {Education,Foundational issues,Hypothesis testing,Inference,Probability},
  number = {sup1},
  pmid = {31413381}
}

@article{lakens_equivalence_2018,
  title = {Equivalence Testing for Psychological Research: {{A}} Tutorial},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  volume = {1},
  pages = {259--269},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@article{lakens_justify_2018,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and Oliveira, Cilene Lino and Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = feb,
  volume = {2},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  annotation = {00010},
  copyright = {2018 The Publisher},
  journal = {Nature Human Behaviour},
  language = {en}
}

@article{miller_quest_2019,
  title = {The Quest for an Optimal Alpha},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2019},
  month = jan,
  volume = {14},
  pages = {e0208631},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0208631},
  abstract = {Researchers who analyze data within the framework of null hypothesis significance testing must choose a critical ``alpha'' level, {$\alpha$}, to use as a cutoff for deciding whether a given set of data demonstrates the presence of a particular effect. In most fields, {$\alpha$} = 0.05 has traditionally been used as the standard cutoff. Many researchers have recently argued for a change to a more stringent evidence cutoff such as {$\alpha$} = 0.01, 0.005, or 0.001, noting that this change would tend to reduce the rate of false positives, which are of growing concern in many research areas. Other researchers oppose this proposed change, however, because it would correspondingly tend to increase the rate of false negatives. We show how a simple statistical model can be used to explore the quantitative tradeoff between reducing false positives and increasing false negatives. In particular, the model shows how the optimal {$\alpha$} level depends on numerous characteristics of the research area, and it reveals that although {$\alpha$} = 0.05 would indeed be approximately the optimal value in some realistic situations, the optimal {$\alpha$} could actually be substantially larger or smaller in other situations. The importance of the model lies in making it clear what characteristics of the research area have to be specified to make a principled argument for using one {$\alpha$} level rather than another, and the model thereby provides a blueprint for researchers seeking to justify a particular {$\alpha$} level.},
  annotation = {00000},
  journal = {PLOS ONE},
  keywords = {Decision theory,Economic growth,Health economics,Medicine and health sciences,Psychology,Publication ethics,Statistical methods,Statistical models},
  language = {en},
  number = {1}
}

@article{oberauer_addressing_2019,
  title = {Addressing the Theory Crisis in Psychology},
  author = {Oberauer, Klaus and Lewandowsky, Stephan},
  year = {2019},
  volume = {26},
  pages = {1596--1618},
  publisher = {{Springer}},
  journal = {Psychonomic bulletin \& review},
  number = {5}
}

@book{ravetz_scientific_1995,
  title = {Scientific {{Knowledge}} and {{Its Social Problems}}},
  author = {Ravetz, Jerome},
  year = {1995},
  month = jan,
  edition = {Reprint edition},
  publisher = {{Transaction Publishers}},
  address = {{New Brunswick, N.J}},
  abstract = {Science is continually confronted by new and difficult social and ethical problems. Some of these problems have arisen from the transformation of the academic science of the prewar period into the industrialized science of the present. Traditional theories of science are now widely recognized as obsolete. In Scientific Knowledge and Its Social Problems (originally published in 1971), Jerome R. Ravetz analyzes the work of science as the creation and investigation of problems. He demonstrates the role of choice and value judgment, and the inevitability of error, in scientific research. Ravetz's new introductory essay is a masterful statement of how our understanding of science has evolved over the last two decades.},
  isbn = {978-1-56000-851-4},
  language = {English}
}

@article{rouder_bayesian_2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  volume = {16},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  annotation = {01475},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {2}
}

@book{zellner_introduction_1971,
  title = {An Introduction to {{Bayesian}} Inference in Econometrics},
  author = {Zellner, Arnold},
  year = {1971},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-98165-7},
  keywords = {Bayesian statistical decision theory,Econometrics},
  lccn = {HB74.M3 Z44},
  series = {Wiley Series in Probability and Mathematical Statistics}
}


@article{skipper_sacredness_1967,
  title = {The sacredness of .05: A note concerning the uses of statistical levels of significance in social science},
  volume = {2},
  issn = {0003-1232},
  shorttitle = {The {{Sacredness}} of .05},
  number = {1},
  journal = {The American Sociologist},
  author = {Skipper, James K. and Guenther, Anthony L. and Nass, Gilbert},
  year = {1967},
  pages = {16-18}
}

@article{cowles_origins_1982,
  title = {On the origins of the. 05 level of statistical significance},
  volume = {37},
  number = {5},
  journal = {American Psychologist},
  author = {Cowles, Michael and Davis, Caroline},
  year = {1982},
  pages = {553-–558}, 
  doi = {10.1037/0003-066X.37.5.553}
}

@article{lindley_statistical_1957,
  title = {A statistical paradox},
  volume = {44},
  number = {1/2},
  journal = {Biometrika},
  author = {Lindley, Dennis V},
  year = {1957},
  pages = {187--192}
}

@article{neyman_problem_1933,
  title = {On the problem of the most efficient tests of statistical hypotheses},
  volume = {231},
  issn = {1364-503X, 1471-2962},
  language = {en},
  number = {694-706},
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  doi = {10.1098/rsta.1933.0009},
  author = {Neyman, J. and Pearson, E. S.},
  month = jan,
  year = {1933},
  pages = {289-337}
}

@book{cohen_statistical_1988,
  address = {{Hillsdale, N.J}},
  edition = {2nd ed},
  title = {Statistical Power Analysis for The Behavioral Sciences},
  isbn = {978-0-8058-0283-2},
  lccn = {HA29 .C66 1988},
  publisher = {{L. Erlbaum Associates}},
  author = {Cohen, Jacob},
  year = {1988},
  keywords = {Statistical methods,Social sciences,Probabilities,Statistical power analysis}
}

@book{leamer_specification_1978,
  address = {{New York}},
  edition = {1 edition},
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  isbn = {978-0-471-01520-8},
  shorttitle = {Specification {{Searches}}},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  language = {English},
  publisher = {{Wiley}},
  author = {Leamer, Edward E.},
  month = apr,
  year = {1978}
}

@article{mudge_setting_2012,
  title = {Setting an optimal {$\alpha$} {{That minimizes errors}} in {{null hypothesis significance tests}}},
  volume = {7},
  issn = {1932-6203},
  abstract = {Null hypothesis significance testing has been under attack in recent years, partly owing to the arbitrary nature of setting {$\alpha$} (the decision-making threshold and probability of Type I error) at a constant value, usually 0.05. If the goal of null hypothesis testing is to present conclusions in which we have the highest possible confidence, then the only logical decision-making threshold is the value that minimizes the probability (or occasionally, cost) of making errors. Setting {$\alpha$} to minimize the combination of Type I and Type II error at a critical effect size can easily be accomplished for traditional statistical tests by calculating the {$\alpha$} associated with the minimum average of {$\alpha$} and {$\beta$} at the critical effect size. This technique also has the flexibility to incorporate prior probabilities of null and alternate hypotheses and/or relative costs of Type I and Type II errors, if known. Using an optimal {$\alpha$} results in stronger scientific inferences because it estimates and minimizes both Type I errors and relevant Type II errors for a test. It also results in greater transparency concerning assumptions about relevant effect size(s) and the relative costs of Type I and II errors. By contrast, the use of {$\alpha$} = 0.05 results in arbitrary decisions about what effect sizes will likely be considered significant, if real, and results in arbitrary amounts of Type II error for meaningful potential effect sizes. We cannot identify a rationale for continuing to arbitrarily use {$\alpha$} = 0.05 for null hypothesis significance tests in any field, when it is possible to determine an optimal {$\alpha$}.},
  number = {2},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0032734},
  author = {Mudge, Joseph F. and Baker, Leanne F. and Edge, Christopher B. and Houlahan, Jeff E.},
  month = feb,
  year = {2012},
  keywords = {Decision making,Experimental design,Freshwater fish,Lakes,Research errors,Gene expression,Shores,Agricultural soil science},
  pages = {e32734}
}

@book{winer_statistical_1962,
  title = {Statistical Principles in Experimental Design},
  abstract = {"Written primarily for students and research workers in the area of the behavioral sciences, this book is meant to provide a text and comprehensive reference source on statistical principles underlying experimental design. Particular emphasis is given to those designs that are likely to prove useful in research in the behavioral sciences. The book primarily emphasizes the logical basis of principles underlying designs for experiments rather than mathematical derivations associated with relevant sampling distributions. The topics selected for inclusion are those covered in courses taught by the author during the past several years. Students in these courses have widely varying backgrounds in mathematics and come primarily from the fields of psychology, education, economics, sociology, and industrial engineering. It has been the intention of the author to keep the book at a readability level appropriate for students having a mathematical background equivalent to freshman college algebra. From experience with those sections of the book which have been used as text material in dittoed form, there is evidence to indicate that, in large measure, the desired readability level has been attained. Admittedly, however, there are some sections in the book where this readability goal has not been achieved. The first course in design, as taught by the author, has as a prerequisite a basic course in statistical inference. The contents of Chaps. 1 and 2 review the highlights of what is included in the prerequisite material. These chapters are not meant to provide the reader with a first exposure to these topics. They are intended to provide a review of terminology and notation for the concepts which are more fully developed in later chapters. By no means is all the material included in the book covered in a one semester course. In a course of this length, the author has included Chaps. 3, 4, parts of 5, 6, parts of 7, parts of 10, and parts of 11. Chapters 8 through 11 were written to be somewhat independent of each other. Hence one may read, with understanding, in these chapters without undue reference to material in the others. In general, the discussion of principles, interpretations of illustrative examples, and computational procedures are included in successive sections within the same chapter. However, to facilitate the use of the book as a reference source, this procedure is not followed in Chaps. 5 and 6. Basic principles associated with a large class of designs for factorial experiments are discussed in Chap. 5. Detailed illustrative examples of these designs are presented in Chap. 6. For teaching purposes, the author includes relevant material from Chap. 6 with the corresponding material in Chap. 5. Selected topics from Chaps. 7 through 11 have formed the basis for a second course in experimental design. Relatively complete tables for sampling distributions of statistics used in the analysis of experimental designs are included in the Appendix. Ample references to source materials having mathematical proofs for the principles stated in the text are provided"--Preface. (PsycINFO Database Record (c) 2008 APA, all rights reserved).  x, 672 p. : ill. ; 24 cm. Social sciences -- Statistical methods. Psychology -- Statistical methods. Research. Plan d'exp{\'e}rience. Experimentelle Psychologie. Statistik. Versuchsplanung. Psychology -- Research. Social sciences -- Research. Experimenteel ontwerp. Variantieanalyse. Experimental design. Statistics as Topic.},
  language = {English},
  publisher = {{New York : McGraw-Hill}},
  author = {Winer, B. J},
  year = {1962}
}

@article{cumming_replication_2008,
  title = {Replication and {\emph{p}} {{Intervals}}: {\emph{P}} {{Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  volume = {3},
  issn = {17456916, 17456924},
  shorttitle = {Replication and {\emph{p}} {{Intervals}}},
  language = {en},
  number = {4},
  journal = {Perspectives on Psychological Science},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  author = {Cumming, Geoff},
  month = jul,
  year = {2008},
  pages = {286-300}
}


@article{lakens2018equivalence,
  title={Equivalence testing for psychological research: A tutorial},
  author={Lakens, Dani{\"e}l and Scheel, Anne M and Isager, Peder M},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={1},
  number={2},
  pages={259--269},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{faulkenberry2019estimating,
  title={Estimating evidential value from analysis of variance summaries: A comment on Ly et al.(2018)},
  author={Faulkenberry, Thomas J},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={2},
  number={4},
  pages={406--409},
  doi = {10.1177/2515245919872960},
  year={2019},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}



@article{tunc_epistemic_2021,
	title = {The epistemic and pragmatic function of dichotomous {Claims} based on statistical hypothesis tests},
	url = {https://psyarxiv.com/af9by/},
	abstract = {Researchers commonly make dichotomous claims based on continuous test statistics. Many have branded the practice as misuse of statistics, and criticize scientists for suffering from “dichotomania”. However, the role dichotomous claims play in science is not primarily a statistical one, but an epistemological and pragmatic one. The epistemological function of dichotomous claims consists in transforming data into factual statements that can falsify a universal statement. This transformation requires pre-specified methodological decision procedures such as statistical hypothesis testing (e.g., Neyman-Pearson tests). From the perspective of methodological falsificationism these decision procedures are necessary, as probabilistic statements (e.g. continuous test statistics) cannot function as falsifiers of substantive hypotheses. However, they are not sufficient since for dichotomous claims to have any implication regarding theoretical claims about phenomena, there should be a valid derivation chain linking theoretical, experimental and data models. The pragmatic function of dichotomous claims is facilitating scrutiny and criticism among peers by generating contestable statements, a process referred to by Popper as 'conjectures and refutations', through which we can determine which theories withstand scrutiny the best. Abandoning dichotomous claims to combat the misuse of statistics would not improve scientific inferences but will sacrifice these crucial epistemic and pragmatic functions.},
	urldate = {2021-02-22},
	author = {Uygun-Tunç, Duygu and Tunç, Mehmet Necip and Lakens, Daniël},
	year = {2021},
	doi = {10.31234/osf.io/af9by},
	note = {type: article},
	keywords = {Quantitative Methods, Social and Behavioral Sciences, Theory and Philosophy of Science, basic statements, dichotomous claims, methodological falsificationism, statisitical hypothesis testing, theory testing},
	file = {Tunç et al_2021_The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\NUIJP6U7\\Tunç et al_2021_The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical.pdf:application/pdf}
}


@article{douglas_inductive_2000,
	title = {Inductive risk and values in science},
	volume = {67},
	number = {4},
	journal = {Philosophy of science},
	author = {Douglas, Heather E.},
	year = {2000},
	note = {Publisher: University of Chicago Press},
	pages = {559--579},
	doi = {10.1086/392855},
	file = {Douglas_2000_Inductive risk and values in science.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\5LNEDQS2\\Douglas_2000_Inductive risk and values in science.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\MI8QQN3U\\392855.html:text/html}
}


@techreport{lakens_sample_2021,
	title = {Sample {Size} {Justification}},
	url = {https://psyarxiv.com/9d3yf/},
	abstract = {An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
	urldate = {2021-01-04},
	institution = {PsyArXiv},
	author = {Lakens, Daniël},
	month = jan,
	year = {2021},
	doi = {10.31234/osf.io/9d3yf},
	note = {type: article},
	keywords = {power analysis, value of information, Experimental Design and Sample Surveys, Quantitative Methods, Social and Behavioral Sciences, sample size justification, study design},
	file = {Lakens_2021_Sample Size Justification.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\ENBKIT5E\\Lakens_2021_Sample Size Justification.pdf:application/pdf}
}


@article{erdfelder_gpower_1996,
	title = {{GPOWER}: {A} general power analysis program},
	volume = {28},
	issn = {0743-3808, 1532-5970},
	shorttitle = {{GPOWER}},
	url = {http://link.springer.com/10.3758/BF03203630},
	doi = {10.3758/BF03203630},
	language = {en},
	number = {1},
	urldate = {2020-08-04},
	journal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
	month = mar,
	year = {1996},
	pages = {1--11},
	file = {Erdfelder et al. - 1996 - GPOWER A general power analysis program.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\8MASB4SB\\Erdfelder et al. - 1996 - GPOWER A general power analysis program.pdf:application/pdf}
}


@book{fisher_design_1971,
	address = {New York},
	edition = {9 edition},
	title = {The {Design} of {Experiments}},
	isbn = {978-0-02-844690-5},
	language = {English},
	publisher = {Macmillan Pub Co},
	author = {Fisher, Ronald A.},
	month = jun,
	year = {1971}
}

@article{kim2021choosing,
  title={Choosing the level of significance: A decision-theoretic approach},
  author={Kim, Jae H and Choi, In},
  journal={Abacus},
  volume={57},
  number={1},
  pages={27--71},
  year={2021},
  doi = {10.1111/abac.12172}
}

@book{edwards_advances_2007,
	address = {Cambridge},
	title = {Advances in decision analysis: {From} foundations to applications},
	isbn = {978-0-521-86368-1},
	shorttitle = {Advances in {Decision} {Analysis}},
	url = {https://www.cambridge.org/core/books/advances-in-decision-analysis/98BD41C9310F6029030B886790DC641F},
	abstract = {Decision analysis is a prescriptive theory that aids individuals or groups confronted with complex problems in a wide variety of contexts. By framing issues, identifying risks, eliciting stakeholder preferences, and suggesting alternative approaches, decision analysts can offer workable solutions in domains such as the environment, health and medicine, engineering and operations research, and public policy. This book is a mixture of historical and forward-looking essays on key topics in decision analysis. Part I covers the history and foundations of decision analysis. Part II discusses structuring decision problems, including the development of objectives and their attributes, and influence diagrams. Part III discusses probabilities and their elicitation and Bayes nets. Part IV discusses additive and multiplicative utilities, risk preferences, and 'option pricing' methods. Part V discusses risk analysis. Part VI puts decision analysis in a behavioral and organizational context. Part VII presents case studies of applications.},
	urldate = {2021-04-02},
	publisher = {Cambridge University Press},
	editor = {Edwards, Ward and Miles Jr., Ralph F. and von Winterfeldt, Detlof},
	year = {2007},
	doi = {10.1017/CBO9780511611308},
	file = {Edwards et al_2007_Advances in Decision Analysis.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\CXR754VV\\Edwards et al_2007_Advances in Decision Analysis.pdf:application/pdf}
}


@book{clemen_making_1997,
	address = {Belmont, Calif},
	edition = {2 edition},
	title = {Making {Hard} {Decisions}: {An} {Introduction} to {Decision} {Analysis}},
	isbn = {978-0-534-26034-7},
	shorttitle = {Making {Hard} {Decisions}},
	abstract = {This best-selling and up-to-date survey of decision analysis concepts and techniques is accessible to students with limited mathematical backgrounds.  It is designed for advanced undergraduate and MBA-level courses in decision analysis and also for business courses in introductory quantitative methods.  (Prerequisites: college algebra; introductory statistics.)},
	language = {English},
	publisher = {Duxbury},
	author = {Clemen, Robert T.},
	month = aug,
	year = {1997},
	file = {Clemen - 1997 - Making Hard Decisions An Introduction to Decision.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\JAP56AR9\\Clemen - 1997 - Making Hard Decisions An Introduction to Decision.pdf:application/pdf}
}


@article{greenland_analysis_2021,
	title = {Analysis goals, error-cost sensitivity, and analysis hacking: {Essential} considerations in hypothesis testing and multiple comparisons},
	volume = {35},
	copyright = {© 2020 The Authors Paediatric and Perinatal Epidemiology © 2020 John Wiley \& Sons Ltd},
	issn = {1365-3016},
	shorttitle = {Analysis goals, error-cost sensitivity, and analysis hacking},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/ppe.12711},
	doi = {https://doi.org/10.1111/ppe.12711},
	abstract = {The “replication crisis” has been attributed to perverse incentives that lead to selective reporting and misinterpretations of P-values and confidence intervals. A crude fix offered for this problem is to lower testing cut-offs (α levels), either directly or in the form of null-biased multiple comparisons procedures such as naïve Bonferroni adjustments. Methodologists and statisticians have expressed positions that range from condemning all such procedures to demanding their application in almost all analyses. Navigating between these unjustifiable extremes requires defining analysis goals precisely enough to separate inappropriate from appropriate adjustments. To meet this need, I here review issues arising in single-parameter inference (such as error costs and loss functions) that are often skipped in basic statistics, yet are crucial to understanding controversies in testing and multiple comparisons. I also review considerations that should be made when examining arguments for and against modifications of decision cut-offs and adjustments for multiple comparisons. The goal is to provide researchers a better understanding of what is assumed by each side and to enable recognition of hidden assumptions. Basic issues of goal specification and error costs are illustrated with simple fixed cut-off hypothesis testing scenarios. These illustrations show how adjustment choices are extremely sensitive to implicit decision costs, making it inevitable that different stakeholders will vehemently disagree about what is necessary or appropriate. Because decisions cannot be justified without explicit costs, resolution of inference controversies is impossible without recognising this sensitivity. Pre-analysis statements of funding, scientific goals, and analysis plans can help counter demands for inappropriate adjustments, and can provide guidance as to what adjustments are advisable. Hierarchical (multilevel) regression methods (including Bayesian, semi-Bayes, and empirical-Bayes methods) provide preferable alternatives to conventional adjustments, insofar as they facilitate use of background information in the analysis model, and thus can provide better-informed estimates on which to base inferences and decisions.},
	language = {en},
	number = {1},
	urldate = {2021-05-22},
	journal = {Paediatric and Perinatal Epidemiology},
	author = {Greenland, Sander},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ppe.12711},
	keywords = {Bonferroni adjustment, cost-benefit analysis, loss functions, multiple comparisons, P-hacking, P-values, replication crisis, sensitivity analysis, significance testing},
	pages = {8--23},
	file = {Greenland_2021_Analysis goals, error-cost sensitivity, and analysis hacking.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\EUIHXFSN\\Greenland_2021_Analysis goals, error-cost sensitivity, and analysis hacking.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\ABLKHWMW\\ppe.html:text/html}
}


@article{wasserstein2016asa,
author = {Ronald L. Wasserstein and Nicole A. Lazar},
title = {The ASA Statement on p-Values: Context, Process, and Purpose},
journal = {The American Statistician},
volume = {70},
number = {2},
pages = {129-133},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2016.1154108},

URL = { 
        https://doi.org/10.1080/00031305.2016.1154108
    
},
eprint = { 
        https://doi.org/10.1080/00031305.2016.1154108
    
}

}

@article{wasserstein2019asa,
author = {Ronald L. Wasserstein and Allen L. Schirm and Nicole A. Lazar},
title = {Moving to a World Beyond “p < 0.05”},
journal = {The American Statistician},
volume = {73},
number = {sup1},
pages = {1-19},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2019.1583913},

URL = { 
        https://doi.org/10.1080/00031305.2019.1583913
    
},
eprint = { 
        https://doi.org/10.1080/00031305.2019.1583913
    
}

}

@article{fisher1956statistical,
  title={Statistical methods and scientific inference.},
  author={Fisher, Ronald A},
  year={1956},
  publisher={Hafner Publishing Co.}
}

@article{ioannidis2005most,
  title={Why most published research findings are false},
  author={Ioannidis, John PA},
  journal={PLoS medicine},
  volume={2},
  number={8},
  pages={e124},
  year={2005},
  publisher={Public Library of Science}, 
  doi = {https://doi.org/10.1371/journal.pmed.0020124}
}

@article{peng2015reproducibility,
  title={The reproducibility crisis in science: A statistical counterattack},
  author={Peng, Roger},
  journal={Significance},
  volume={12},
  number={3},
  pages={30--32},
  year={2015},
  publisher={Wiley Online Library}, 
  doi = {https://doi.org/10.1111/j.1740-9713.2015.00827.x}
}

@article{simmons2011false,
  title={False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant},
  author={Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
  journal={Psychological science},
  volume={22},
  number={11},
  pages={1359--1366},
  year={2011},
  publisher={Sage Publications Sage CA: Los Angeles, CA}, 
  doi = {https://doi.org/10.1177%2F0956797611417632}
}

@article{open2015estimating,
  title={Estimating the reproducibility of psychological science},
  author={Open Science Collaboration and others},
  journal={Science},
  volume={349},
  number={6251},
  year={2015},
  publisher={American Association for the Advancement of Science}, 
  doi = {https://doi.org/10.1126/science.aac4716}
}

@article{kass1995bayes,
  title={Bayes factors},
  author={Kass, Robert E and Raftery, Adrian E},
  journal={Journal of the american statistical association},
  volume={90},
  number={430},
  pages={773--795},
  year={1995},
  publisher={Taylor \& Francis}
}

@BOOK{LeeWagenmakersBayesBook,
  AUTHOR =       {Lee, M. D. and Wagenmakers, Eric-Jan},
  TITLE =        {Bayesian Cognitive Modeling: {A} Practical Course},
  PUBLISHER =    {Cambridge University Press},
  YEAR =         {2013}
}

@BOOK{Jeffreys1939,
  AUTHOR =       {Jeffreys, H.},
  TITLE =        {Theory of Probability},
  PUBLISHER =    {Oxford University Press},
  YEAR =         {1939},
  address =      {Oxford, UK},
  edition =      {1}
}

@article{kim2020decision,
  title={Decision-theoretic hypothesis testing: A primer with R package OptSig},
  author={Kim, Jae H},
  journal={The American Statistician},
  volume={74},
  number={4},
  pages={370--379},
  year={2020},
  publisher={Taylor \& Francis}, 
  doi = {https://doi.org/10.1080/00031305.2020.1750484}
}

@article{bayarri2016rejection,
  title={Rejection odds and rejection ratios: A proposal for statistical practice in testing hypotheses},
  author={Bayarri, MJ and Benjamin, Daniel J and Berger, James O and Sellke, Thomas M},
  journal={Journal of Mathematical Psychology},
  volume={72},
  pages={90--103},
  year={2016},
  publisher={Elsevier}, 
  url = {https://doi.org/10.1016/j.jmp.2015.12.007}
}

@misc{degroot1975probability,
  title={Probability and Statistics. Massachusetts},
  author={DeGroot, MH},
  year={1975},
  publisher={Addison-Wesley Publishing Company, InC}
}

@article{pericchi2016adaptative,
  title={Adaptative significance levels using optimal decision rules: balancing by weighting the error probabilities},
  author={Pericchi, Luis and Pereira, Carlos},
  journal={Brazilian Journal of Probability and Statistics},
  volume={30},
  number={1},
  pages={70--90},
  year={2016},
  publisher={Brazilian Statistical Association}
}

@article{lindley1953statistical,
  title={Statistical inference},
  author={Lindley, Dennis V},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={15},
  number={1},
  pages={30--65},
  year={1953},
  publisher={Wiley Online Library}
}

@inproceedings{jeffreys1935some,
  title={Some tests of significance, treated by the theory of probability},
  author={Jeffreys, Harold},
  booktitle={Mathematical Proceedings of the Cambridge Philosophical Society},
  volume={31},
  number={2},
  pages={203--222},
  year={1935},
  organization={Cambridge University Press}
}

@article{bartlett1957comment,
  title={comment on D. V. Lindley’s statistical paradox},
  author={Bartlett, Peter L and Jordan, Michael I and Mcauliffe, Jon D},
  booktitle={V. Lindley’s statistical paradox, Biometrika},
  year={1957},
  journal = {Biometrika},
  volume = {44}, 
  pages = {533-534}
}

@article{jeffreys1936on,
  title={On some criticisms of the theory of probability},
  author={Jeffreys, Harold},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={22},
  number={146},
  pages={337--359},
  year={1936},
  publisher={Taylor \& Francis}
}

@inproceedings{jeffreys1936further,
  title={Further significance tests},
  author={Jeffreys, Harold},
  booktitle={Mathematical Proceedings of the Cambridge Philosophical Society},
  volume={32},
  number={3},
  pages={416--445},
  year={1936},
  organization={Cambridge University Press}
}

@article{WagenmakersJeffreys,
 title = {History and Nature of the Jeffreys-Lindley Paradox}, 
 author = {Wagenmakers, E.J. and Ly, Alexander}
}

@article{lin2013research,
  title={Research commentary—too big to fail: large samples and the p-value problem},
  author={Lin, Mingfeng and Lucas Jr, Henry C and Shmueli, Galit},
  journal={Information Systems Research},
  volume={24},
  number={4},
  pages={906--917},
  year={2013},
  publisher={INFORMS}
}

@article{cornfield1966,
  title={Sequential trials, sequential analysis and the likelihood principle},
  author={Cornfield, Jerome},
  journal={The American Statistician},
  volume={20},
  number={2},
  pages={18--23},
  year={1966}
}

@article{harford2014big,
  title={Big data: A big mistake?},
  author={Harford, Tim},
  journal={Significance},
  volume={11},
  number={5},
  pages={14--19},
  year={2014},
  url ={https://doi.org/10.1111/j.1740-9713.2014.00778.x}
}

@article{pennycook2020fighting,
  title={Fighting COVID-19 misinformation on social media: Experimental evidence for a scalable accuracy-nudge intervention},
  author={Pennycook, Gordon and McPhetres, Jonathon and Zhang, Yunhao and Lu, Jackson G and Rand, David G},
  journal={Psychological Science},
  volume={31},
  number={7},
  pages={770--780},
  year={2020}, 
  url = {https://doi.org/10.1177/0956797620939054}
}

@article{viamonte2006cost,
  title={A cost-benefit analysis of risk-reduction strategies targeted at older drivers},
  author={Viamonte, Sarah M and Ball, Karlene K and Kilgore, Meredith},
  journal={Traffic Injury Prevention},
  volume={7},
  number={4},
  pages={352--359},
  year={2006},
  publisher={Taylor \& Francis}, 
  url = {https://doi.org/10.1080/15389580600791362}
}

@article{wald1949statistical,
  title={Statistical decision functions},
  author={Wald, Abraham},
  journal={The Annals of Mathematical Statistics},
  pages={165--205},
  year={1949},
  publisher={JSTOR}, 
  url = {https://doi.org/10.1214/aoms/1177730030}
}

@article{cornfield1969bayesian,
  title={The Bayesian outlook and its application},
  author={Cornfield, Jerome},
  journal={Biometrics},
  pages={617--657},
  year={1969}
}


